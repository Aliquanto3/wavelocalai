#!/usr/bin/env python3
"""
WaveLocalAI - Benchmark SLM Complet
===================================
Script d'√©valuation approfondie des Small Language Models (SLM) locaux et API.

M√©triques mesur√©es :
- Performance d'inf√©rence (tokens/s, TTFT, dur√©e)
- Consommation de ressources (RAM, VRAM)
- Impact environnemental (CO2 via CodeCarbon)
- Capacit√©s fonctionnelles (tools, JSON, multilingue, raisonnement)
- Robustesse du contexte (needle-in-haystack)

Auteur: WaveLocalAI Team
Version: 2.0.0
"""

import argparse
import contextlib
import csv
import json
import logging
import random
import re
import shutil
import string
import sys
import time
from datetime import datetime
from pathlib import Path

import psutil

# Ajout du path pour les imports internes
ROOT_DIR = Path(__file__).parent.parent
sys.path.append(str(ROOT_DIR))

try:
    import ollama
    from codecarbon import OfflineEmissionsTracker

    from src.core.config import (
        BENCHMARKS_DIR,
        DEFAULT_COUNTRY_ISO_CODE,
        EMISSIONS_DIR,
        MISTRAL_API_KEY,
        MODELS_JSON_PATH,
    )
except ImportError as e:
    print(f"‚ùå Erreur d'import : {e}")
    print("Assurez-vous d'avoir install√© : ollama, codecarbon, python-dotenv")
    sys.exit(1)

# Import optionnel pour l'API Mistral
try:
    from mistralai import Mistral

    MISTRAL_AVAILABLE = True
except ImportError:
    MISTRAL_AVAILABLE = False


# --- CONFIGURATION ---
BENCHMARKS_DIR.mkdir(parents=True, exist_ok=True)
REPORT_MD_PATH = BENCHMARKS_DIR / "benchmark_report.md"
DATASET_CSV_PATH = BENCHMARKS_DIR / "benchmarks_data.csv"
AUDIT_LOG_FILE = BENCHMARKS_DIR / f"benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

# Seuils et param√®tres
MIN_RAM_MARGIN_GB = 1.0
CONTEXT_LEVELS = [2048, 4096, 8192, 16384, 32768, 65536, 131072]
TOTAL_RAM_GB = round(psutil.virtual_memory().total / (1024**3), 2)
DEFAULT_OUTPUT_TOKENS = 256
DEFAULT_TIMEOUT_S = 300
DEFAULT_RUNS = 1

# Langues support√©es pour les tests multilingues (Listes de synonymes accept√©s)
SUPPORTED_LANGUAGES = {
    "en": {"name": "English", "hello": "Hello", "expected": ["hello", "hi", "greetings"]},
    "fr": {"name": "French", "hello": "Bonjour", "expected": ["bonjour", "salut", "coucou"]},
    "es": {"name": "Spanish", "hello": "Hola", "expected": ["hola", "buenos d√≠as", "buenas"]},
    "de": {"name": "German", "hello": "Hallo", "expected": ["hallo", "guten tag", "hi"]},
    "it": {"name": "Italian", "hello": "Ciao", "expected": ["ciao", "salve", "buongiorno"]},
    "pt": {"name": "Portuguese", "hello": "Ol√°", "expected": ["ol√°", "oi", "bom dia"]},
    "zh": {"name": "Chinese", "hello": "‰Ω†Â•Ω", "expected": ["‰Ω†Â•Ω", "ÊÇ®Â•Ω"]},
    "ja": {"name": "Japanese", "hello": "„Åì„Çì„Å´„Å°„ÅØ", "expected": ["„Åì„Çì„Å´„Å°„ÅØ", "„Éè„É≠„Éº"]},
    "ko": {"name": "Korean", "hello": "ÏïàÎÖïÌïòÏÑ∏Ïöî", "expected": ["ÏïàÎÖïÌïòÏÑ∏Ïöî", "ÏïàÎÖï"]},
    "ar": {"name": "Arabic", "hello": "ŸÖÿ±ÿ≠ÿ®ÿß", "expected": ["ŸÖÿ±ÿ≠ÿ®ÿß", "ÿ£ŸáŸÑÿß", "ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ"]},
    "ru": {"name": "Russian", "hello": "–ü—Ä–∏–≤–µ—Ç", "expected": ["–ø—Ä–∏–≤–µ—Ç", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ"]},
}

# Tests de raisonnement logique
REASONING_TESTS = [
    {
        "id": "logic_syllogism",
        "prompt": "All roses are flowers. All flowers need water. Does a rose need water? Answer only 'yes' or 'no'.",
        "expected": "yes",
        "category": "logic",
    },
    {
        "id": "arithmetic_simple",
        "prompt": "What is 17 + 28? Answer with only the number.",
        "expected": "45",
        "category": "arithmetic",
    },
    {
        "id": "arithmetic_multi",
        "prompt": "If I have 3 boxes with 4 apples each, and I eat 2 apples, how many apples are left? Answer with only the number.",
        "expected": "10",
        "category": "arithmetic",
    },
    {
        "id": "logic_negation",
        "prompt": "If it is NOT true that all cats are black, can there be a white cat? Answer only 'yes' or 'no'.",
        "expected": "yes",
        "category": "logic",
    },
    {
        "id": "sequence",
        "prompt": "What is the next number in this sequence: 2, 4, 8, 16, ? Answer with only the number.",
        "expected": "32",
        "category": "pattern",
    },
]

# Tests de suivi d'instructions
INSTRUCTION_TESTS = [
    {
        "id": "format_list",
        "prompt": "List exactly 3 colors. Format: one color per line, no numbers, no punctuation.",
        "check": lambda r: len([line for line in r.strip().split("\n") if line.strip()]) == 3,
    },
    {
        "id": "format_uppercase",
        "prompt": "Write the word 'hello' in uppercase letters only.",
        "check": lambda r: "HELLO" in r.upper() and r.strip().isupper(),
    },
    {
        "id": "format_json_simple",
        "prompt": 'Return a valid JSON object with exactly two keys: "name" (string) and "age" (number). Nothing else.',
        "check": lambda r: _is_valid_json_with_keys(r, ["name", "age"]),
    },
    {
        "id": "constraint_length",
        "prompt": "Describe the sun in exactly 10 words. Count carefully.",
        "check": lambda r: 8 <= len(r.split()) <= 12,  # Tol√©rance ¬±2
    },
]

# Sch√©ma JSON pour le test de conformit√©
JSON_SCHEMA_TEST = {
    "prompt": """Generate a JSON object representing a person with the following structure:
{
  "firstName": string,
  "lastName": string,
  "age": integer (between 0 and 120),
  "email": string (valid email format),
  "active": boolean
}
Return ONLY the JSON, no explanation.""",
    "schema": {
        "type": "object",
        "required": ["firstName", "lastName", "age", "email", "active"],
        "properties": {
            "firstName": {"type": "string"},
            "lastName": {"type": "string"},
            "age": {"type": "integer", "minimum": 0, "maximum": 120},
            "email": {"type": "string", "pattern": r"^[\w\.-]+@[\w\.-]+\.\w+$"},
            "active": {"type": "boolean"},
        },
    },
}

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(AUDIT_LOG_FILE, encoding="utf-8"),
        logging.StreamHandler(sys.stdout),
    ],
)
logger = logging.getLogger(__name__)


# --- HELPERS ---
def _is_valid_json_with_keys(response: str, required_keys: list) -> bool:
    """V√©rifie si la r√©ponse est un JSON valide avec les cl√©s requises."""
    try:
        # Nettoyage des backticks markdown
        clean = re.sub(r"```json\s*|\s*```", "", response.strip())
        data = json.loads(clean)
        return all(k in data for k in required_keys)
    except (json.JSONDecodeError, TypeError):
        return False


def _validate_json_schema(response: str, schema: dict) -> tuple[bool, bool]:
    """
    Valide un JSON contre un sch√©ma simplifi√©.
    Retourne (json_valide, schema_conforme).
    """
    try:
        clean = re.sub(r"```json\s*|\s*```", "", response.strip())
        data = json.loads(clean)
    except (json.JSONDecodeError, TypeError):
        return False, False

    # V√©rification basique du sch√©ma
    props = schema.get("properties", {})
    required = schema.get("required", [])

    # V√©rifier les champs requis
    if not all(k in data for k in required):
        return True, False

    # V√©rifier les types
    for key, spec in props.items():
        if key not in data:
            continue
        value = data[key]
        expected_type = spec.get("type")

        if expected_type == "string" and not isinstance(value, str):
            return True, False
        if expected_type == "integer" and not isinstance(value, int):
            return True, False
        if expected_type == "boolean" and not isinstance(value, bool):
            return True, False

        # V√©rifier les contraintes num√©riques
        if expected_type == "integer":
            if "minimum" in spec and value < spec["minimum"]:
                return True, False
            if "maximum" in spec and value > spec["maximum"]:
                return True, False

        # V√©rifier le pattern regex
        if "pattern" in spec and isinstance(value, str) and not re.match(spec["pattern"], value):
            return True, False

    return True, True


def detect_license_type(model_tag: str) -> str:
    """
    R√©cup√®re la licence via Ollama et tente de la classifier.
    Retourne : 'Apache 2.0', 'MIT', 'Llama Community', etc.
    """
    try:
        # R√©cup√©ration des m√©tadonn√©es brutes
        info = ollama.show(model_tag)
        license_text = info.get("license", "").lower()

        if not license_text:
            return "Non d√©tect√©e"

        # Heuristiques de classification
        if "apache" in license_text and "2.0" in license_text:
            return "Apache 2.0"
        if "mit license" in license_text:
            return "MIT"
        if "llama" in license_text and "community" in license_text:
            return "Llama Community"
        if "creative commons" in license_text:
            if "nc" in license_text or "noncommercial" in license_text:
                return "CC-BY-NC"
            return "CC-BY"
        if "openrail" in license_text:
            return "OpenRAIL"

        return "Autre (Voir d√©tails)"
    except Exception:
        return "Erreur lecture"


def _get_ux_rating(ttft_ms: float) -> str:
    """Note qualitative de la fluidit√© (Latence)."""
    if ttft_ms <= 0:
        return "N/A"
    if ttft_ms < 300:
        return "‚ö° Instantan√©"
    if ttft_ms < 800:
        return "üöÄ Rapide"
    if ttft_ms < 1500:
        return "üê¢ Acceptable"
    return "üêå Lent"


def _get_efficiency_score(reasoning_score: float, co2_per_1k: float) -> str:
    """
    Score d'efficience : Rapport entre l'intelligence (Reasoning) et le co√ªt (CO2).
    Permet d'identifier les mod√®les 'intelligents pour leur poids carbone'.
    """
    if co2_per_1k <= 0:
        return "N/A"

    # Conversion en grammes pour lisibilit√© calcul
    co2_g = co2_per_1k * 1000
    if co2_g == 0:
        return "N/A"

    # Formule : (Score Raisonnement 0-1 * 100) / (Grammes CO2 par 1k tokens)
    # Exemple : Score 0.8 (80%) / 0.1g CO2 = Ratio 800
    score = (reasoning_score * 100) / co2_g

    if score > 500:
        return "üü¢ Excellent"
    if score > 200:
        return "üü° Bon"
    return "üî¥ Faible"


def _generate_needle_haystack(
    context_size: int, depth_percent: float = 0.5
) -> tuple[str, str, str]:
    """
    G√©n√®re un test needle-in-haystack.
    depth_percent: Position de l'aiguille (0.0 = d√©but, 0.5 = milieu, 1.0 = fin)
    Retourne (prompt_complet, needle, question).
    """
    # G√©n√©rer un code secret unique
    secret_code = "".join(random.choices(string.ascii_uppercase + string.digits, k=8))
    needle = f"The secret code is: {secret_code}"

    # Calculer la taille du filler (environ 4 chars/token en moyenne)
    target_chars = (context_size * 3) // 4
    filler_paragraphs = []

    topics = [
        "The history of ancient civilizations reveals fascinating patterns of human development.",
        "Modern technology continues to transform how we communicate and work.",
        "Environmental science studies the complex interactions between organisms and their habitats.",
        "Economic theories attempt to explain the behavior of markets and consumers.",
        "Philosophical inquiry examines fundamental questions about existence and knowledge.",
        "Medical research advances our understanding of human health and disease.",
        "Architectural design balances aesthetic beauty with functional requirements.",
        "Literary analysis explores the deeper meanings within written works.",
        "Musical composition combines rhythm, melody, and harmony in creative ways.",
        "Agricultural practices have evolved significantly over thousands of years.",
    ]

    while len(" ".join(filler_paragraphs)) < target_chars:
        filler_paragraphs.append(random.choice(topics) * 3)

    # Ins√©rer le needle √† la position demand√©e
    insert_idx = int(len(filler_paragraphs) * depth_percent)
    insert_idx = max(0, min(insert_idx, len(filler_paragraphs)))  # Bornage

    filler_paragraphs.insert(insert_idx, needle)

    haystack = "\n\n".join(filler_paragraphs)

    prompt = f"""Read the following text carefully and find the secret code hidden within it.

TEXT:
{haystack}

QUESTION: What is the secret code mentioned in the text above? Answer with only the code, nothing else."""

    return prompt, secret_code, "secret_code"


# --- GESTION CSV ---
def get_csv_headers() -> list[str]:
    """Retourne les headers du CSV avec toutes les colonnes."""
    base_headers = [
        "model_name",
        "ollama_tag",
        "model_type",
        "disk_size_gb",
        "context_size",
        "input_tokens",
        "output_tokens",
        "tokens_per_second",
        "time_to_first_token_ms",
        "duration_s",
        "ollama_ram_usage_gb",
        "gpu_vram_usage_gb",
        "ram_peak_gb",
        "co2_kg",
        "co2_per_1k_tokens",
        "status",
        "tool_call_valid",
        "tool_call_function_correct",
        "tool_call_params_correct",
        "json_generation_valid",
        "json_schema_compliant",
        "needle_in_haystack_found",
    ]

    # Ajouter les colonnes de langues
    for lang_code in SUPPORTED_LANGUAGES:
        base_headers.append(f"lang_{lang_code}_comprehension")
        base_headers.append(f"lang_{lang_code}_generation")

    # Ajouter les scores de qualit√©
    base_headers.extend(
        [
            "reasoning_score",
            "instruction_following_score",
            "response_variance",
            "run_id",
            "date",
        ]
    )

    return base_headers


def initialize_csv():
    """Cr√©e le fichier CSV avec les headers s'il n'existe pas."""
    if not DATASET_CSV_PATH.exists():
        headers = get_csv_headers()
        with open(DATASET_CSV_PATH, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(headers)
        logger.info(f"üìÑ CSV initialis√© : {DATASET_CSV_PATH}")


def append_to_csv(results: list[dict]):
    """Ajoute une liste de r√©sultats √† la fin du CSV."""
    if not results:
        return

    headers = get_csv_headers()

    with open(DATASET_CSV_PATH, "a", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=headers, extrasaction="ignore")
        for result in results:
            # S'assurer que toutes les colonnes existent
            row = {h: result.get(h, "") for h in headers}
            writer.writerow(row)


# --- OUTILS SYST√àME ---
def unload_model(model_tag: str):
    """D√©charge un mod√®le de la m√©moire Ollama."""
    try:
        ollama.chat(model=model_tag, messages=[], keep_alive=0)
        time.sleep(2)
    except Exception:
        pass


def get_system_ram_stats() -> tuple[float, float]:
    """Retourne (RAM utilis√©e, RAM disponible) en GB."""
    mem = psutil.virtual_memory()
    return mem.used / (1024**3), mem.available / (1024**3)


def get_ollama_process_memory_gb() -> float:
    """Mesure la RAM utilis√©e par les processus Ollama."""
    total_mem_bytes = 0
    try:
        for proc in psutil.process_iter(["name", "memory_info"]):
            try:
                if "ollama" in proc.info["name"].lower():
                    total_mem_bytes += proc.info["memory_info"].rss
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
                pass
    except Exception:
        pass
    return total_mem_bytes / (1024**3)


def get_gpu_memory_usage_gb() -> float:
    """Tente de mesurer l'utilisation VRAM GPU (NVIDIA)."""
    try:
        import subprocess

        result = subprocess.run(
            [
                "nvidia-smi",
                "--query-gpu=memory.used",
                "--format=csv,noheader,nounits",
            ],
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode == 0:
            # Somme de toutes les GPU
            total_mb = sum(int(x) for x in result.stdout.strip().split("\n") if x)
            return total_mb / 1024
    except Exception:
        pass
    return 0.0


def get_real_disk_size(model_tag: str) -> float:
    """R√©cup√®re la taille r√©elle sur disque d'un mod√®le Ollama."""
    try:
        list_info = ollama.list()
        for m in list_info.get("models", []):
            if m["model"] == model_tag or m["model"] == f"{model_tag}:latest":
                return round(m["size"] / (1024**3), 2)
    except Exception:
        pass
    return 0.0


def check_model_installed(model_tag: str) -> str | None:
    """V√©rifie si un mod√®le est install√© et retourne son tag exact."""
    try:
        installed = {m["model"]: m for m in ollama.list().get("models", [])}
        if model_tag in installed:
            return model_tag
        if f"{model_tag}:latest" in installed:
            return f"{model_tag}:latest"
    except Exception:
        pass
    return None


# --- TESTS FONCTIONNELS ---
class ModelTester:
    """Classe encapsulant tous les tests pour un mod√®le."""

    def __init__(
        self,
        model_tag: str,
        model_type: str = "local",
        country_iso: str = DEFAULT_COUNTRY_ISO_CODE,
    ):
        self.model_tag = model_tag
        self.model_type = model_type
        self.country_iso = country_iso
        self.mistral_client = None

        if model_type == "api" and MISTRAL_AVAILABLE and MISTRAL_API_KEY:
            self.mistral_client = Mistral(api_key=MISTRAL_API_KEY)

    def _call_model(
        self,
        messages: list[dict],
        tools: list | None = None,
        options: dict | None = None,
    ) -> dict | None:
        """Appelle le mod√®le (local ou API) et retourne la r√©ponse."""
        if self.model_type == "api":
            return self._call_api(messages, tools)
        return self._call_local(messages, tools, options)

    def _call_local(
        self,
        messages: list[dict],
        tools: list | None = None,
        options: dict | None = None,
    ) -> dict | None:
        """Appelle un mod√®le local via Ollama avec logging d√©taill√©."""
        try:
            # LOGGING: Entr√©e
            input_text = messages[-1].get("content", "")
            logger.info(
                f"\n   üì• [INPUT] {self.model_tag}:\n{input_text[:300]}{'...' if len(input_text)>300 else ''}"
            )

            kwargs = {"model": self.model_tag, "messages": messages}
            if tools:
                kwargs["tools"] = tools
            if options:
                kwargs["options"] = options

            resp = ollama.chat(**kwargs)

            content = resp.message.content
            tool_calls = getattr(resp.message, "tool_calls", None)

            # LOGGING: Sortie
            log_content = content if content else (str(tool_calls) if tool_calls else "<Empty>")
            logger.info(
                f"   üì§ [OUTPUT] {self.model_tag}:\n{log_content[:300]}{'...' if len(log_content)>300 else ''}\n"
            )

            return {
                "content": content,
                "tool_calls": tool_calls,
                "prompt_eval_count": getattr(resp, "prompt_eval_count", 0) or 0,
                "eval_count": getattr(resp, "eval_count", 0) or 0,
            }
        except Exception as e:
            logger.error(f"Erreur appel local: {e}")
            return None

    def _call_api(self, messages: list[dict], tools: list | None = None) -> dict | None:
        """Appelle l'API Mistral avec logging d√©taill√©."""
        if not self.mistral_client:
            return None

        try:
            # LOGGING: Entr√©e
            input_text = messages[-1].get("content", "")
            logger.info(
                f"\n   üì• [API INPUT] {self.model_tag}:\n{input_text[:300]}{'...' if len(input_text)>300 else ''}"
            )

            kwargs = {"model": self.model_tag, "messages": messages}
            if tools:
                kwargs["tools"] = tools

            resp = self.mistral_client.chat.complete(**kwargs)
            choice = resp.choices[0].message

            content = choice.content or ""
            tool_calls = getattr(choice, "tool_calls", None)

            # LOGGING: Sortie
            log_content = content if content else (str(tool_calls) if tool_calls else "<Empty>")
            logger.info(
                f"   üì§ [API OUTPUT] {self.model_tag}:\n{log_content[:300]}{'...' if len(log_content)>300 else ''}\n"
            )

            return {
                "content": choice.content or "",
                "tool_calls": getattr(choice, "tool_calls", None),
                "prompt_eval_count": getattr(resp.usage, "prompt_tokens", 0),
                "eval_count": getattr(resp.usage, "completion_tokens", 0),
            }
        except Exception as e:
            logger.error(f"Erreur appel API: {e}")
            return None

    def test_tool_calling(self) -> dict:
        """Test de capacit√© tool calling avec validation compl√®te."""
        result = {"valid": False, "function_correct": False, "params_correct": False}

        test_tool = {
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Get current weather for a location",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "City name",
                        }
                    },
                    "required": ["location"],
                },
            },
        }

        messages = [
            {
                "role": "user",
                "content": "What is the current weather in Paris, France?",
            }
        ]

        resp = self._call_model(
            messages, tools=[test_tool], options={"temperature": 0, "num_ctx": 2048}
        )

        if not resp or not resp.get("tool_calls"):
            return result

        result["valid"] = True

        tool_call = resp["tool_calls"][0]
        func = getattr(tool_call, "function", tool_call)

        func_name = getattr(func, "name", None) or func.get("name")
        if func_name == "get_weather":
            result["function_correct"] = True

        args = getattr(func, "arguments", None) or func.get("arguments", {})
        if isinstance(args, str):
            try:
                args = json.loads(args)
            except json.JSONDecodeError:
                args = {}

        if "location" in args:
            location = args["location"].lower()
            if "paris" in location:
                result["params_correct"] = True

        return result

    def test_tool_not_calling(self) -> bool:
        """Test que le mod√®le ne call PAS un tool quand ce n'est pas pertinent."""
        test_tool = {
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Get current weather for a location",
                "parameters": {
                    "type": "object",
                    "properties": {"location": {"type": "string"}},
                    "required": ["location"],
                },
            },
        }
        messages = [{"role": "user", "content": "What is the capital of France?"}]
        resp = self._call_model(
            messages, tools=[test_tool], options={"temperature": 0, "num_ctx": 2048}
        )
        if not resp:
            return False
        return not resp.get("tool_calls")

    def test_json_generation(self) -> tuple[bool, bool]:
        """Test de g√©n√©ration JSON avec validation de sch√©ma."""
        messages = [{"role": "user", "content": JSON_SCHEMA_TEST["prompt"]}]
        resp = self._call_model(messages, options={"temperature": 0, "num_ctx": 2048})
        if not resp or not resp.get("content"):
            return False, False
        return _validate_json_schema(resp["content"], JSON_SCHEMA_TEST["schema"])

    def test_language(self, lang_code: str) -> dict:
        """Test de support linguistique avec tol√©rance accrue."""
        result = {"comprehension": False, "generation": False}
        lang_info = SUPPORTED_LANGUAGES.get(lang_code)

        if not lang_info:
            return result

        # Test de g√©n√©ration : Traduction
        if lang_code != "en":
            gen_prompt = f"Translate the English word 'Hello' into {lang_info['name']}. Return ONLY the translated word."
            resp = self._call_model(
                [{"role": "user", "content": gen_prompt}],
                options={"temperature": 0, "num_ctx": 2048},
            )

            if resp and resp.get("content"):
                content = resp["content"].strip().lower()
                # Nettoyage ponctuation
                content = re.sub(r"[^\w\s]", "", content)
                expected_list = lang_info.get("expected", [])

                # V√©rification : au moins un des mots attendus est pr√©sent
                if any(exp in content for exp in expected_list):
                    result["generation"] = True
                elif len(content) > 0 and "hello" not in content:
                    # Heuristique fallback
                    result["generation"] = True

        # Test de compr√©hension : Question simple
        # Mise √† jour des r√©ponses attendues (listes √©largies)
        expected_answers = {
            "en": ["blue", "azure", "clear"],
            "fr": ["bleu", "bleue", "azur", "claire"],
            "es": ["azul", "celeste", "claro"],
            "de": ["blau", "himmelblau", "klar"],
            "it": ["blu", "azzurro", "celeste"],
            "pt": ["azul", "celeste", "claro"],
            "zh": ["Ëìù", "Â§©Ëìù", "Èùí"],
            "ja": ["Èùí", "„Éñ„É´„Éº", "Ê∞¥Ëâ≤"],
            "ko": ["ÌååÎûÄ", "ÌååÎûë", "Ìë∏Î•∏", "ÌïòÎäòÏÉâ"],
            "ar": ["ÿ£ÿ≤ÿ±ŸÇ", "ÿ≤ÿ±ŸÇÿßÿ°", "ÿ≥ŸÖÿßŸàŸä"],
            "ru": ["–≥–æ–ª—É–±–æ–π", "—Å–∏–Ω–∏–π", "–ª–∞–∑—É—Ä–Ω—ã–π"],
        }

        comp_prompts = {
            "en": "What color is the sky on a clear day? Answer in one word.",
            "fr": "De quelle couleur est le ciel par temps clair ? R√©pondez en un mot.",
            "es": "¬øDe qu√© color es el cielo en un d√≠a despejado? Responde en una palabra.",
            "de": "Welche Farbe hat der Himmel an einem klaren Tag? Antworte mit einem Wort.",
            "it": "Di che colore √® il cielo in una giornata limpida? Rispondi con una parola.",
            "pt": "Qual √© a cor do c√©u em um dia claro? Responda em uma palavra.",
            "zh": "Êô¥Â§©Êó∂Â§©Á©∫ÊòØ‰ªÄ‰πàÈ¢úËâ≤ÔºüÁî®‰∏Ä‰∏™ËØçÂõûÁ≠î„ÄÇ",
            "ja": "Êô¥„Çå„ÅüÊó•„ÅÆÁ©∫„ÅØ‰ΩïËâ≤„Åß„Åô„ÅãÔºü‰∏ÄË®Ä„ÅßÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
            "ko": "ÎßëÏùÄ ÎÇ† ÌïòÎäòÏùÄ Î¨¥Ïä® ÏÉâÏù∏Í∞ÄÏöî? Ìïú Îã®Ïñ¥Î°ú ÎåÄÎãµÌïòÏÑ∏Ïöî.",
            "ar": "ŸÖÿß ŸÑŸàŸÜ ÿßŸÑÿ≥ŸÖÿßÿ° ŸÅŸä ŸäŸàŸÖ ÿµÿßŸÅŸçÿü ÿ£ÿ¨ÿ® ÿ®ŸÉŸÑŸÖÿ© Ÿàÿßÿ≠ÿØÿ©.",
            "ru": "–ö–∞–∫–æ–≥–æ —Ü–≤–µ—Ç–∞ –Ω–µ–±–æ –≤ —è—Å–Ω—ã–π –¥–µ–Ω—å? –û—Ç–≤–µ—Ç—å—Ç–µ –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º.",
        }

        if lang_code in comp_prompts:
            resp = self._call_model(
                [{"role": "user", "content": comp_prompts[lang_code]}],
                options={"temperature": 0, "num_ctx": 2048},
            )

            if resp and resp.get("content"):
                content = resp["content"].strip().lower()
                expected_list = expected_answers.get(lang_code, [])
                if any(exp in content for exp in expected_list):
                    result["comprehension"] = True

        return result

    def test_reasoning(self) -> float:
        """Ex√©cute les tests de raisonnement et retourne un score 0-1."""
        correct = 0
        total = len(REASONING_TESTS)

        for test in REASONING_TESTS:
            resp = self._call_model(
                [{"role": "user", "content": test["prompt"]}],
                options={"temperature": 0, "num_ctx": 2048},
            )

            if resp and resp.get("content"):
                content = resp["content"].strip().lower()
                expected = test["expected"].lower()
                if expected in content or content == expected:
                    correct += 1

        return round(correct / total, 2) if total > 0 else 0.0

    def test_instruction_following(self) -> float:
        """Ex√©cute les tests de suivi d'instructions."""
        correct = 0
        total = len(INSTRUCTION_TESTS)
        for test in INSTRUCTION_TESTS:
            resp = self._call_model(
                [{"role": "user", "content": test["prompt"]}],
                options={"temperature": 0, "num_ctx": 2048},
            )
            if resp and resp.get("content"):
                try:
                    if test["check"](resp["content"]):
                        correct += 1
                except Exception:
                    pass
        return round(correct / total, 2) if total > 0 else 0.0

    def test_needle_in_haystack(self, context_size: int) -> bool:
        """
        Test needle-in-haystack robuste (D√©but, Milieu, Fin).
        Le test r√©ussit si le mod√®le trouve l'aiguille dans TOUTES les positions.
        """
        positions = [0.1, 0.5, 0.9]  # 10%, 50%, 90%
        success_count = 0

        logger.info(
            f"         üîç Needle-in-haystack ({context_size}): Testing {len(positions)} positions..."
        )

        for pos in positions:
            prompt, expected_code, _ = _generate_needle_haystack(context_size, depth_percent=pos)

            resp = self._call_model(
                [{"role": "user", "content": prompt}],
                options={"temperature": 0, "num_ctx": context_size},
            )

            found = False
            if resp and resp.get("content"):
                content = resp["content"].strip().upper()
                if expected_code in content:
                    found = True

            pos_label = f"{int(pos*100)}%"
            if found:
                success_count += 1
                logger.debug(f"            Pos {pos_label}: ‚úÖ Found")
            else:
                logger.warning(f"            Pos {pos_label}: ‚ùå Failed (Code: {expected_code})")

        # R√®gle stricte : Doit r√©ussir partout pour √™tre consid√©r√© "fiable"
        # Vous pouvez relaxer ceci en retournant success_count >= 2 par exemple
        is_robust = success_count == len(positions)
        return is_robust


# --- BENCHMARK PRINCIPAL ---
def benchmark_inference(
    model_tag: str,
    context_window: int,
    model_type: str = "local",
    country_iso: str = DEFAULT_COUNTRY_ISO_CODE,
    output_token_limit: int = DEFAULT_OUTPUT_TOKENS,
) -> dict | None:
    """
    Benchmark d'inf√©rence avec mesures compl√®tes.
    """
    if model_type == "local":
        unload_model(model_tag)

        _, sys_ram_avail = get_system_ram_stats()
        if sys_ram_avail < MIN_RAM_MARGIN_GB:
            logger.warning(f"   ‚ö†Ô∏è  RAM insuffisante ({sys_ram_avail:.2f}GB dispo)")
            return None

    ollama_ram_start = get_ollama_process_memory_gb() if model_type == "local" else 0
    gpu_vram_start = get_gpu_memory_usage_gb()

    # Initialiser CodeCarbon
    EMISSIONS_DIR.mkdir(parents=True, exist_ok=True)
    tracker = None
    if model_type == "local":
        tracker = OfflineEmissionsTracker(
            country_iso_code=country_iso,
            output_dir=str(EMISSIONS_DIR),
            log_level="error",
        )
        tracker.start()

    start_time = time.perf_counter()
    first_token_time = None

    messages = [
        {
            "role": "user",
            "content": "Write a detailed technical explanation about how neural networks learn through backpropagation.",
        }
    ]

    try:
        if model_type == "local":
            # Mode streaming pour mesurer TTFT
            stream = ollama.chat(
                model=model_tag,
                messages=messages,
                stream=True,
                options={
                    "num_ctx": context_window,
                    "temperature": 0.7,
                    "num_predict": output_token_limit,
                },
            )

            response_content = ""
            input_tokens = 0
            output_tokens = 0

            for chunk in stream:
                if first_token_time is None:
                    first_token_time = time.perf_counter()

                if hasattr(chunk, "message") and chunk.message.content:
                    response_content += chunk.message.content

                # R√©cup√©rer les stats √† la fin
                if hasattr(chunk, "prompt_eval_count"):
                    input_tokens = chunk.prompt_eval_count or 0
                if hasattr(chunk, "eval_count"):
                    output_tokens = chunk.eval_count or 0

            if output_tokens == 0:
                output_tokens = len(response_content.split())

        else:
            # Mode API (pas de streaming pour simplifier)
            if not MISTRAL_AVAILABLE or not MISTRAL_API_KEY:
                logger.warning("   ‚ö†Ô∏è  API Mistral non configur√©e")
                return None

            client = Mistral(api_key=MISTRAL_API_KEY)
            first_token_time = time.perf_counter()  # Approximation

            resp = client.chat.complete(
                model=model_tag,
                messages=messages,
                max_tokens=output_token_limit,
            )

            input_tokens = resp.usage.prompt_tokens
            output_tokens = resp.usage.completion_tokens
            response_content = resp.choices[0].message.content

        duration = time.perf_counter() - start_time
        ttft_ms = int((first_token_time - start_time) * 1000) if first_token_time else 0

        # Mesures post-inf√©rence
        if model_type == "local":
            ollama_ram_end = get_ollama_process_memory_gb()
            sys_ram_peak, _ = get_system_ram_stats()
            emissions = tracker.stop() if tracker else 0
            ram_model_usage = max(0, ollama_ram_end - ollama_ram_start)
        else:
            sys_ram_peak = 0
            emissions = 0  # Pas de mesure CO2 pour API
            ram_model_usage = 0

        gpu_vram_end = get_gpu_memory_usage_gb()
        gpu_vram_usage = max(0, gpu_vram_end - gpu_vram_start)

        # Calculer tokens/s
        tokens_per_second = round(output_tokens / duration, 2) if duration > 0 else 0

        # CO2 par 1000 tokens
        total_tokens = input_tokens + output_tokens
        co2_per_1k = round((emissions / total_tokens) * 1000, 8) if total_tokens > 0 else 0

        status = "OK"
        if output_tokens >= output_token_limit * 0.95:
            status = "MAX_TOKENS_HIT"

        if model_type == "local":
            unload_model(model_tag)

        return {
            "context_size": context_window,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "tokens_per_second": tokens_per_second,
            "time_to_first_token_ms": ttft_ms,
            "duration_s": round(duration, 2),
            "ollama_ram_usage_gb": round(ram_model_usage, 3),
            "gpu_vram_usage_gb": round(gpu_vram_usage, 3),
            "ram_peak_gb": round(sys_ram_peak, 2),
            "co2_kg": float(emissions),
            "co2_per_1k_tokens": co2_per_1k,
            "status": status,
        }

    except Exception as e:
        if tracker:
            with contextlib.suppress(Exception):
                tracker.stop()
        logger.error(f"   ‚ùå Erreur benchmark: {e}")
        return None


def run_full_benchmark(
    model_name: str,
    model_data: dict,
    args: argparse.Namespace,
    run_id: int = 1,
) -> list[dict]:
    """
    Ex√©cute le benchmark complet pour un mod√®le.
    Retourne une liste de r√©sultats (un par niveau de contexte).
    """
    model_tag = model_data.get("ollama_tag")
    model_type = model_data.get("type", "local")
    max_model_ctx = model_data.get("ctx", 4096)
    disk_size = model_data.get("size_gb", "0")

    # Nettoyer la taille
    if isinstance(disk_size, str):
        disk_size = disk_size.replace(" GB", "").replace("‚âà", "")
        try:
            disk_size = float(disk_size)
        except ValueError:
            disk_size = 0

    results = []
    tester = ModelTester(model_tag, model_type, args.country)

    # --- TESTS FONCTIONNELS (une seule fois) ---
    logger.info("   üîß Tests fonctionnels...")

    # Tool calling
    tool_results = {"valid": False, "function_correct": False, "params_correct": False}
    if "tools" in model_data.get("capabilities", []) or args.force_tool_test:
        tool_results = tester.test_tool_calling()
        logger.info(
            f"      Tools: valid={tool_results['valid']}, func={tool_results['function_correct']}, params={tool_results['params_correct']}"
        )

    # JSON
    json_valid, json_schema = tester.test_json_generation()
    logger.info(f"      JSON: valid={json_valid}, schema={json_schema}")

    # Langues
    # Langues (Test syst√©matique de toutes les langues support√©es par le benchmark)
    lang_results = {}
    # On ignore model_data.get("langs") pour forcer la v√©rification r√©elle
    for lang_code in SUPPORTED_LANGUAGES:
        lang_results[lang_code] = tester.test_language(lang_code)

    supported_langs = [
        lc for lc, res in lang_results.items() if res["comprehension"] or res["generation"]
    ]
    logger.info(f"      Langues support√©es: {supported_langs}")

    # Raisonnement
    reasoning_score = tester.test_reasoning()
    logger.info(f"      Raisonnement: {reasoning_score * 100:.0f}%")

    # Suivi d'instructions
    instruction_score = tester.test_instruction_following()
    logger.info(f"      Instructions: {instruction_score * 100:.0f}%")

    # --- TESTS DE CONTEXTE ---
    logger.info("   üìä Tests de mont√©e en contexte...")

    prev_ram_usage = 0.0
    context_levels = [c for c in CONTEXT_LEVELS if c <= max_model_ctx]

    if args.max_context:
        context_levels = [c for c in context_levels if c <= args.max_context]

    for ctx in context_levels:
        logger.info(f"      ‚ö° Contexte {ctx}...")

        # Benchmark d'inf√©rence
        bench = benchmark_inference(
            model_tag,
            ctx,
            model_type,
            args.country,
            args.output_tokens,
        )

        if not bench:
            logger.warning(f"      ‚ùå √âchec √† ctx={ctx}")
            # Enregistrer l'√©chec
            results.append(
                {
                    "model_name": model_name,
                    "ollama_tag": model_tag,
                    "model_type": model_type,
                    "disk_size_gb": disk_size,
                    "context_size": ctx,
                    "status": "FAILED",
                    "run_id": run_id,
                    "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                }
            )
            break

        # D√©tection du swap (local seulement)
        if model_type == "local":
            current_ram = bench["ollama_ram_usage_gb"]
            if prev_ram_usage > 0 and current_ram < prev_ram_usage * 0.8:
                logger.warning("      üìâ SWAP d√©tect√©! Arr√™t.")
                bench["status"] = "SWAP_DETECTED"
                # On garde ce r√©sultat mais on arr√™te
                results.append(
                    _build_result_row(
                        model_name,
                        model_tag,
                        model_type,
                        disk_size,
                        bench,
                        tool_results,
                        json_valid,
                        json_schema,
                        lang_results,
                        reasoning_score,
                        instruction_score,
                        False,
                        run_id,
                    )
                )
                break
            prev_ram_usage = current_ram

        # Test needle-in-haystack
        needle_found = False
        if ctx >= 4096:  # Seulement pour contextes >= 4K
            needle_found = tester.test_needle_in_haystack(ctx)
            logger.info(f"         Needle-in-haystack: {'‚úÖ' if needle_found else '‚ùå'}")

        # Construire le r√©sultat
        result_row = _build_result_row(
            model_name,
            model_tag,
            model_type,
            disk_size,
            bench,
            tool_results,
            json_valid,
            json_schema,
            lang_results,
            reasoning_score,
            instruction_score,
            needle_found,
            run_id,
        )

        results.append(result_row)

        status_icon = "‚úÖ" if bench["status"] == "OK" else "‚ö†Ô∏è"
        logger.info(
            f"      {status_icon} {bench['status']} | "
            f"{bench['tokens_per_second']} tok/s | "
            f"TTFT {bench['time_to_first_token_ms']}ms | "
            f"RAM {bench['ollama_ram_usage_gb']}GB"
        )

    return results


def _build_result_row(
    model_name: str,
    model_tag: str,
    model_type: str,
    disk_size: float,
    bench: dict,
    tool_results: dict,
    json_valid: bool,
    json_schema: bool,
    lang_results: dict,
    reasoning_score: float,
    instruction_score: float,
    needle_found: bool,
    run_id: int,
) -> dict:
    """Construit une ligne de r√©sultat compl√®te."""
    row = {
        "model_name": model_name,
        "ollama_tag": model_tag,
        "model_type": model_type,
        "disk_size_gb": disk_size,
        "context_size": bench.get("context_size", 0),
        "input_tokens": bench.get("input_tokens", 0),
        "output_tokens": bench.get("output_tokens", 0),
        "tokens_per_second": bench.get("tokens_per_second", 0),
        "time_to_first_token_ms": bench.get("time_to_first_token_ms", 0),
        "duration_s": bench.get("duration_s", 0),
        "ollama_ram_usage_gb": bench.get("ollama_ram_usage_gb", 0),
        "gpu_vram_usage_gb": bench.get("gpu_vram_usage_gb", 0),
        "ram_peak_gb": bench.get("ram_peak_gb", 0),
        "co2_kg": bench.get("co2_kg", 0),
        "co2_per_1k_tokens": bench.get("co2_per_1k_tokens", 0),
        "status": bench.get("status", "UNKNOWN"),
        "tool_call_valid": tool_results.get("valid", False),
        "tool_call_function_correct": tool_results.get("function_correct", False),
        "tool_call_params_correct": tool_results.get("params_correct", False),
        "json_generation_valid": json_valid,
        "json_schema_compliant": json_schema,
        "needle_in_haystack_found": needle_found,
        "reasoning_score": reasoning_score,
        "instruction_following_score": instruction_score,
        "response_variance": 0.0,  # TODO: impl√©menter avec multi-runs
        "run_id": run_id,
        "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    }

    # Ajouter les r√©sultats de langues
    for lang_code in SUPPORTED_LANGUAGES:
        lang_res = lang_results.get(lang_code, {})
        row[f"lang_{lang_code}_comprehension"] = lang_res.get("comprehension", False)
        row[f"lang_{lang_code}_generation"] = lang_res.get("generation", False)

    return row


def update_model_json(db: dict, model_name: str, results: list[dict]):
    """Met √† jour le JSON avec les statistiques r√©sum√©es et les nouvelles m√©triques."""
    if not results:
        return

    # 1. Filtrage et moyennes de base
    ok_results = [r for r in results if r.get("status") == "OK"]
    if not ok_results:
        ok_results = results

    best = max(ok_results, key=lambda x: x.get("context_size", 0))

    avg_tps = sum(r.get("tokens_per_second", 0) for r in ok_results) / len(ok_results)
    avg_ttft = sum(r.get("time_to_first_token_ms", 0) for r in ok_results) / len(ok_results)
    total_co2 = sum(r.get("co2_kg", 0) for r in results)
    avg_co2_per_1k = sum(r.get("co2_per_1k_tokens", 0) for r in ok_results) / len(ok_results)

    # Scores de qualit√©
    reasoning_avg = best.get("reasoning_score", 0)
    instruction_avg = best.get("instruction_following_score", 0)

    # 2. Calcul des nouvelles m√©triques intelligentes
    ux_rating = _get_ux_rating(avg_ttft)
    eff_score = _get_efficiency_score(reasoning_avg, avg_co2_per_1k)

    # 3. D√©tection automatique de la licence (Ollama uniquement)
    model_tag = db[model_name].get("ollama_tag", "")
    detected_license = "N/A"
    if db[model_name].get("type") == "local":
        detected_license = detect_license_type(model_tag)

    # 4. Construction de l'objet stats complet
    stats = {
        "date": datetime.now().strftime("%Y-%m-%d"),
        "max_validated_ctx": best.get("context_size", 0),
        "ram_usage_at_max_ctx_gb": best.get("ollama_ram_usage_gb", 0),
        "gpu_vram_usage_gb": best.get("gpu_vram_usage_gb", 0),
        # Performance pure
        "avg_tokens_per_second": round(avg_tps, 2),
        "avg_ttft_ms": int(avg_ttft),
        # M√©triques D√©cisionnelles (Nouvelles)
        "ux_rating": ux_rating,  # ‚ö° Instantan√©, üöÄ Rapide...
        "efficiency_grade": eff_score,  # üü¢ Excellent, üî¥ Faible...
        "detected_license": detected_license,  # Apache 2.0, MIT...
        # RSE
        "total_co2_emissions_kg": total_co2,
        "avg_co2_per_1k_tokens": round(avg_co2_per_1k, 6),
        # Capacit√©s
        "tool_capability": {
            "function_detection": best.get("tool_call_valid", False),
            "parameter_extraction": best.get("tool_call_params_correct", False),
            "success_rate": 1.0 if best.get("tool_call_params_correct") else 0.0,
        },
        "json_capability": {
            "valid_json_rate": 1.0 if best.get("json_generation_valid") else 0.0,
            "schema_compliance_rate": 1.0 if best.get("json_schema_compliant") else 0.0,
        },
        "needle_in_haystack": {},
        "quality_scores": {
            "reasoning_avg": reasoning_avg,
            "instruction_following_avg": instruction_avg,
            "response_variance_avg": best.get("response_variance", 0),
        },
    }

    # Needle-in-haystack
    for r in results:
        ctx = r.get("context_size", 0)
        if ctx >= 4096:
            stats["needle_in_haystack"][f"ctx_{ctx // 1024}k"] = r.get(
                "needle_in_haystack_found", False
            )

    # Langues
    languages_supported = {}
    for lang_code in SUPPORTED_LANGUAGES:
        comp = best.get(f"lang_{lang_code}_comprehension", False)
        gen = best.get(f"lang_{lang_code}_generation", False)
        if comp or gen:
            languages_supported[lang_code] = {"comprehension": comp, "generation": gen}

    # Mise √† jour DB
    db[model_name]["benchmark_stats"] = stats
    if languages_supported:
        db[model_name]["languages_validated"] = languages_supported

    # Capabilities Validated (Tags automatiques)
    validated_caps = []
    if best.get("tool_call_params_correct"):
        validated_caps.append("tools_validated")
    if best.get("json_schema_compliant"):
        validated_caps.append("json_validated")

    # Tag automatique multilingue
    if len(languages_supported) >= 5:
        validated_caps.append("multilingual_full")
    elif len(languages_supported) >= 2:
        validated_caps.append("multilingual_partial")

    # Tag automatique licence "Open" (si d√©tect√©)
    if detected_license in ["Apache 2.0", "MIT"]:
        validated_caps.append("license_permissive")

    if validated_caps:
        db[model_name]["capabilities_validated"] = validated_caps


def _get_ux_rating(ttft_ms: float) -> str:
    """Retourne une note visuelle pour l'UX bas√©e sur le TTFT."""
    if ttft_ms < 300:
        return "‚ö° Instantan√©"
    if ttft_ms < 800:
        return "üöÄ Rapide"
    if ttft_ms < 1500:
        return "üê¢ Acceptable"
    return "üêå Lent"


def _get_efficiency_score(reasoning_score: float, co2_per_1k: float) -> str:
    """Calcule un score d'efficience (Qualit√© / CO2)."""
    if co2_per_1k <= 0:
        return "N/A"
    # Formule arbitraire pour le score : (Reasoning * 100) / (CO2_g * 10)
    # Plus le score est haut, plus le mod√®le est "intelligent pour son co√ªt carbone"
    co2_g = co2_per_1k * 1000
    if co2_g == 0:
        return "N/A"

    score = (reasoning_score * 100) / co2_g

    if score > 500:
        return "üü¢ Excellent"
    if score > 200:
        return "üü° Bon"
    return "üî¥ Faible"


def generate_markdown_report(db: dict):
    """G√©n√®re un rapport Markdown enrichi g√©rant les ex-aequo."""
    logger.info(f"üìù G√©n√©ration du rapport enrichi : {REPORT_MD_PATH}")

    # Pr√©paration des donn√©es
    models_data = []
    for name, data in db.items():
        stats = data.get("benchmark_stats", {})
        if not stats:
            continue

        # Calculs d√©riv√©s
        ttft = stats.get("avg_ttft_ms", 0)
        reasoning = stats.get("quality_scores", {}).get("reasoning_avg", 0)
        co2 = stats.get("avg_co2_per_1k_tokens", 0)

        models_data.append(
            {
                "name": name,
                "type": data.get("type", "local"),
                "size": data.get("size_gb", "?"),
                "ctx": stats.get("max_validated_ctx", 0),
                "tps": stats.get("avg_tokens_per_second", 0),
                "ttft": ttft,
                "co2": co2,
                "reasoning": reasoning,
                "tools": stats.get("tool_capability", {}).get("success_rate", 0) > 0.8,
                "json": stats.get("json_capability", {}).get("schema_compliance_rate", 0) > 0.8,
                "langs": len(data.get("languages_validated", {})),
                "efficiency": _get_efficiency_score(reasoning, co2),
                "ux_rating": _get_ux_rating(ttft),
            }
        )

    if not models_data:
        logger.warning("‚ö†Ô∏è Pas de donn√©es pour g√©n√©rer le rapport.")
        return

    # --- GESTION DES CHAMPIONS (AVEC EX-AEQUO) ---

    def get_winners(data, key, reverse=True, filter_func=None):
        """Retourne la liste des mod√®les ayant le meilleur score."""
        candidates = [d for d in data if filter_func(d)] if filter_func else data
        if not candidates:
            return [], 0

        # Tri pour trouver le meilleur score
        sorted_data = sorted(candidates, key=lambda x: x[key], reverse=reverse)
        best_score = sorted_data[0][key]

        # R√©cup√©ration de tous les ex-aequo
        winners = [d for d in sorted_data if d[key] == best_score]
        return winners, best_score

    # 1. Raisonnement (Score le plus haut)
    top_reasoning, best_reasoning_score = get_winners(models_data, "reasoning", reverse=True)

    # 2. Vitesse (TPS le plus haut)
    top_speed, best_tps_score = get_winners(models_data, "tps", reverse=True)

    # 3. Frugalit√© (CO2 le plus bas, strictement positif)
    top_eco, best_eco_score = get_winners(
        models_data, "co2", reverse=False, filter_func=lambda x: x["co2"] > 0
    )

    # Helper pour formater les noms
    def format_names(models_list):
        names = [f"**{m['name']}**" for m in models_list]
        return ", ".join(names)

    # --- CONSTRUCTION DU RAPPORT ---
    lines = [
        "# üìä WaveLocalAI - Rapport d'Aide √† la D√©cision\n",
        f"> **Date du rapport** : {datetime.now().strftime('%d/%m/%Y √† %H:%M')}\n",
        f"> **Contexte** : {len(models_data)} mod√®les √©valu√©s sur {TOTAL_RAM_GB} GB RAM\n",
        "\n---\n",
        "## üèÜ Synth√®se Ex√©cutive (Executive Summary)\n",
        "Ce r√©sum√© identifie les mod√®les les plus performants selon vos priorit√©s m√©tier.\n\n",
    ]

    # Bloc Intelligence
    title_perf = "Les plus intelligents" if len(top_reasoning) > 1 else "Le plus intelligent"
    lines.extend(
        [
            f"### üß† {title_perf} (Capacit√©s cognitives)\n",
            f"{format_names(top_reasoning) if top_reasoning else 'N/A'}\n",
            (
                f"- Score de raisonnement : **{best_reasoning_score*100:.0f}%**\n"
                if top_reasoning
                else ""
            ),
            "- *Recommand√© pour : Agents complexes, RAG, Analyse de documents.*\n\n",
        ]
    )

    # Bloc Vitesse
    title_speed = "Les plus rapides" if len(top_speed) > 1 else "Le plus rapide"
    # On prend l'UX rating du premier (ils sont suppos√©s proches si tps proche, sinon on prend celui du 1er)
    ux_lbl = top_speed[0]["ux_rating"] if top_speed else ""
    lines.extend(
        [
            f"### ‚ö° {title_speed} (Exp√©rience Utilisateur)\n",
            f"{format_names(top_speed) if top_speed else 'N/A'}\n",
            f"- Vitesse : **{best_tps_score:.0f} tok/s** ({ux_lbl})\n" if top_speed else "",
            "- *Recommand√© pour : Chatbots temps r√©el, Auto-compl√©tion.*\n\n",
        ]
    )

    # Bloc √âcologie
    title_eco = "Les plus frugaux" if len(top_eco) > 1 else "Le plus frugal"
    lines.extend(
        [
            f"### üå± {title_eco} (RSE & Green IT)\n",
            f"{format_names(top_eco) if top_eco else 'N/A'}\n",
            f"- Impact : **{best_eco_score*1000:.4f} gCO‚ÇÇ** / 1k tokens\n" if top_eco else "",
            "- *Recommand√© pour : Traitement de fond (batch), IoT, Usage massif.*\n\n",
        ]
    )

    lines.extend(
        [
            "\n---\n",
            "## üö¶ Matrice de D√©cision\n",
            "Comparatif global pour orienter le choix technique.\n",
            "| Mod√®le | Type | Taille | UX (Latence) | Raisonnement | Efficience RSE | Tools | JSON |\n",
            "|--------|------|--------|--------------|--------------|----------------|-------|------|\n",
        ]
    )

    for m in models_data:
        tools_icon = "‚úÖ" if m["tools"] else "‚ùå"
        json_icon = "‚úÖ" if m["json"] else "‚ùå"
        reasoning_str = f"{m['reasoning']*100:.0f}%"

        lines.append(
            f"| **{m['name'][:25]}** | {m['type']} | {m['size']} | {m['ux_rating']} | {reasoning_str} | {m['efficiency']} | {tools_icon} | {json_icon} |\n"
        )

    lines.extend(
        [
            "\n---\n",
            "## üåç Analyse d'Impact Environnemental (D√©tail)\n",
            "Focus sur la consommation √©nerg√©tique relative des mod√®les.\n",
            "| Mod√®le | Contexte Max Valid√© | √âmissions (gCO‚ÇÇ/1k tok) | √âquivalent |\n",
            "|--------|---------------------|-------------------------|------------|\n",
        ]
    )

    for m in models_data:
        co2_g = m["co2"] * 1000
        equiv = f"{co2_g/4:.2f} emails" if co2_g > 0 else "?"
        lines.append(f"| {m['name']} | {m['ctx']} tokens | **{co2_g:.4f} g** | {equiv} |\n")

    lines.extend(
        [
            "\n---\n",
            "## üìã Annexe Technique\n",
            "- **UX Rating** : Bas√© sur le TTFT (Time To First Token). <300ms est imperceptible.\n",
            "- **Efficience RSE** : Ratio entre la qualit√© de r√©ponse et le co√ªt carbone.\n",
            "- **M√©thodologie** : Tests r√©alis√©s en local via Ollama + CodeCarbon.\n",
            "\n*G√©n√©r√© par WaveLocalAI v2.1*",
        ]
    )

    with open(REPORT_MD_PATH, "w", encoding="utf-8") as f:
        f.writelines(lines)


# --- MAIN ---
def main():
    parser = argparse.ArgumentParser(
        description="üî¨ WaveLocalAI - Benchmark SLM Complet",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  python benchmark_slm.py                           # Tous les mod√®les locaux
  python benchmark_slm.py --models qwen2.5:0.5b     # Un mod√®le sp√©cifique
  python benchmark_slm.py --type api                # Seulement les mod√®les API
  python benchmark_slm.py --type all --no-update    # Tous types, sans MAJ JSON
  python benchmark_slm.py --skip-tested             # Ignorer les d√©j√† test√©s
        """,
    )

    # Filtrage des mod√®les
    parser.add_argument(
        "--models",
        "-m",
        nargs="+",
        help="Tags Ollama des mod√®les √† tester (ex: qwen2.5:0.5b llama3.2:1b)",
    )
    parser.add_argument(
        "--type",
        "-t",
        choices=["local", "api", "all"],
        default="local",
        help="Type de mod√®les √† tester (d√©faut: local)",
    )
    parser.add_argument(
        "--skip-tested",
        action="store_true",
        help="Ignorer les mod√®les ayant d√©j√† des benchmark_stats",
    )

    # Configuration des tests
    parser.add_argument(
        "--max-context",
        type=int,
        help="Contexte maximum √† tester (ex: 8192)",
    )
    parser.add_argument(
        "--output-tokens",
        type=int,
        default=DEFAULT_OUTPUT_TOKENS,
        help=f"Nombre de tokens √† g√©n√©rer (d√©faut: {DEFAULT_OUTPUT_TOKENS})",
    )
    parser.add_argument(
        "--runs",
        type=int,
        default=DEFAULT_RUNS,
        help=f"Nombre de runs par mod√®le pour la variance (d√©faut: {DEFAULT_RUNS})",
    )
    parser.add_argument(
        "--country",
        default=DEFAULT_COUNTRY_ISO_CODE,
        help=f"Code ISO pays pour CodeCarbon (d√©faut: {DEFAULT_COUNTRY_ISO_CODE})",
    )

    # Options de tests
    parser.add_argument(
        "--force-tool-test",
        action="store_true",
        help="Forcer le test tool calling m√™me si non d√©clar√©",
    )
    parser.add_argument(
        "--force-lang-test",
        action="store_true",
        help="Tester toutes les langues m√™me si non d√©clar√©es",
    )
    parser.add_argument(
        "--skip-functional",
        action="store_true",
        help="Passer les tests fonctionnels (tools, JSON, langues)",
    )

    # Outputs
    parser.add_argument(
        "--no-update",
        "-n",
        action="store_true",
        help="Ne pas mettre √† jour le fichier models.json",
    )
    parser.add_argument(
        "--no-report",
        action="store_true",
        help="Ne pas g√©n√©rer le rapport Markdown",
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Mode verbeux (debug)",
    )

    parser.add_argument(
        "--report-only",
        action="store_true",
        help="G√©n√©rer uniquement le rapport Markdown depuis models.json sans lancer de benchmark",
    )

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # Charger la base de mod√®les
    if not MODELS_JSON_PATH.exists():
        logger.error(f"‚ùå Fichier introuvable : {MODELS_JSON_PATH}")
        sys.exit(1)

    with open(MODELS_JSON_PATH, encoding="utf-8") as f:
        db = json.load(f)

    # Backup du JSON
    if not args.no_update:
        shutil.copy(MODELS_JSON_PATH, str(MODELS_JSON_PATH) + ".bak")

    # --- LOGIQUE REPORT-ONLY ---
    if args.report_only:
        logger.info("üìÑ Mode rapport seul activ√©.")
        generate_markdown_report(db)
        logger.info(f"‚úÖ Rapport r√©g√©n√©r√© : {REPORT_MD_PATH}")
        sys.exit(0)
    # ---------------------------

    # V√©rifier Ollama pour les mod√®les locaux
    if args.type in ("local", "all"):
        try:
            ollama.list()
        except Exception:
            logger.error("‚ùå Ollama non d√©tect√©. Lancez 'ollama serve'.")
            if args.type == "local":
                sys.exit(1)

    # V√©rifier API Mistral
    if args.type in ("api", "all"):
        if not MISTRAL_AVAILABLE:
            logger.warning("‚ö†Ô∏è  Package mistralai non install√©")
        if not MISTRAL_API_KEY:
            logger.warning("‚ö†Ô∏è  MISTRAL_API_KEY non configur√©e")

    # Initialiser le CSV
    initialize_csv()

    # Filtrer les mod√®les √† tester
    models_to_test = []
    for name, data in db.items():
        model_type = data.get("type", "local")
        tag = data.get("ollama_tag")

        # Filtrer par type
        if args.type != "all" and model_type != args.type:
            continue

        # Filtrer par tags sp√©cifiques
        if args.models and tag not in args.models:
            continue

        # Ignorer les d√©j√† test√©s
        if args.skip_tested and data.get("benchmark_stats"):
            logger.info(f"‚è≠Ô∏è  {name} : D√©j√† test√©, ignor√©.")
            continue

        # V√©rifier l'installation (local seulement)
        if model_type == "local":
            real_tag = check_model_installed(tag)
            if not real_tag:
                logger.warning(f"‚è© {name} ({tag}) : Non install√©, ignor√©.")
                continue
            data["_real_tag"] = real_tag
        else:
            data["_real_tag"] = tag

        models_to_test.append((name, data))

    if not models_to_test:
        logger.warning("‚ö†Ô∏è  Aucun mod√®le √† tester.")
        sys.exit(0)

    logger.info(f"üöÄ Benchmark de {len(models_to_test)} mod√®le(s)")
    logger.info(f"üñ•Ô∏è  RAM syst√®me : {TOTAL_RAM_GB} GB")
    logger.info(f"üåç Pays CodeCarbon : {args.country}")

    # Ex√©cuter les benchmarks
    for i, (name, data) in enumerate(models_to_test, 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"[{i}/{len(models_to_test)}] üî¨ {name}")
        logger.info(f"{'='*60}")

        try:
            all_results = []

            for run_id in range(1, args.runs + 1):
                if args.runs > 1:
                    logger.info(f"   üìç Run {run_id}/{args.runs}")

                results = run_full_benchmark(name, data, args, run_id)
                all_results.extend(results)

            # Sauvegarder dans le CSV
            append_to_csv(all_results)
            logger.info(f"   üíæ {len(all_results)} r√©sultats sauv√©s dans CSV")

            # Mettre √† jour le JSON
            if not args.no_update and all_results:
                update_model_json(db, name, all_results)
                with open(MODELS_JSON_PATH, "w", encoding="utf-8") as f:
                    json.dump(db, f, indent=4, ensure_ascii=False)
                logger.info("   üìù JSON mis √† jour")

        except KeyboardInterrupt:
            logger.warning("\n‚ö†Ô∏è  Interruption utilisateur")
            break
        except Exception as e:
            logger.error(f"   ‚ùå Erreur : {e}")
            if args.verbose:
                import traceback

                traceback.print_exc()
            continue

    # G√©n√©rer le rapport
    if not args.no_report:
        generate_markdown_report(db)

    logger.info("\n‚úÖ Benchmark termin√© !")
    logger.info(f"   üìä CSV : {DATASET_CSV_PATH}")
    logger.info(f"   üìù Rapport : {REPORT_MD_PATH}")
    logger.info(f"   üìã Logs : {AUDIT_LOG_FILE}")


if __name__ == "__main__":
    main()
