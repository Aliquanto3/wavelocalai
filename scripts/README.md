# üìÇ Scripts d'Administration & Maintenance

Ce r√©pertoire contient les outils en ligne de commande (CLI) permettant de g√©rer le cycle de vie de l'application WaveLocalAI, de la configuration des mod√®les √† l'audit de performance avanc√©.

## üìã Pr√©requis

Tous les scripts doivent √™tre ex√©cut√©s depuis la **racine du projet** pour que les imports Python (`src.core...`) fonctionnent correctement. Assurez-vous d'utiliser l'environnement virtuel du projet.

**Exemple d'ex√©cution standard :**
```bash
# Windows
.venv\Scripts\python scripts/nom_du_script.py

# Mac/Linux
.venv/bin/python scripts/nom_du_script.py
```

---

## üì¶ 1. Gestionnaire de Mod√®les (`setup_models.py`)

Synchronise votre instance locale Ollama avec le fichier de configuration central `data/models.json`.

**Fonctionnalit√©s :**
* D√©tection automatique des mod√®les manquants.
* T√©l√©chargement avec barre de progression.
* V√©rification des conflits de tags.

**Arguments :**

| Argument | Description |
| :--- | :--- |
| `(aucun)` | Installe tous les mod√®les manquants d√©finis dans le JSON. |
| `--dry-run` | **Simulation :** Affiche ce qui serait install√© sans rien t√©l√©charger. Utile pour v√©rifier l'√©tat. |
| `--force` | **R√©paration :** Force le re-t√©l√©chargement m√™me si le mod√®le existe d√©j√† (utile en cas de fichier GGUF corrompu). |

---

## üìà 2. Audit & Green Benchmark (`audit_and_update.py`)

Un outil avanc√© d'√©valuation ("Eval Ops") qui r√©alise un **Stress Test progressif** sur chaque mod√®le install√©.

**Ce que fait le script :**
1.  **V√©rification Technique :** Tente un *Tool Call* r√©el pour v√©rifier si le mod√®le est compatible "Agent".
2.  **Stress Test Progressif (Ramp-up) :**
    * Teste des contextes croissants : **2k, 8k, 16k, 32k, 64k**.
    * **S√©curit√© RAM :** V√©rifie la RAM syst√®me disponible avant chaque palier. Arr√™te l'escalade si la marge de s√©curit√© (< 2 GB) est atteinte pour √©viter le crash du PC.
    * **D√©tection de boucles :** Identifie si un mod√®le est trop "bavard" ou boucle, mais continue les tests de performance si la RAM le permet.
3.  **M√©triques Pr√©cises :**
    * **RAM Mod√®le :** Isole la consommation sp√©cifique des processus Ollama (ex: 0.5 GB).
    * **System Peak :** Mesure la charge totale du PC (ex: 14/32 GB).
    * **Empreinte Carbone :** Mesure via **CodeCarbon**.
4.  **Exports et Rapports :**
    * Mise √† jour de `data/models.json` avec l'historique hi√©rarchique.
    * G√©n√©ration d'un rapport lisible : `data/benchmark_report.md`.
    * **Nouveau :** G√©n√©ration d'un dataset plat pour analyse (Data Science) : `data/benchmarks_data.csv`.

**Utilisation :**
Ce script ne prend pas d'arguments obligatoires. Il it√®re sur tous les mod√®les pr√©sents dans `models.json`.
*Note : Le benchmark peut prendre du temps car il teste plusieurs contextes par mod√®le.*

```bash
python scripts/audit_and_update.py
```

**Options disponibles :**

| Option | Description |
| :--- | :--- |
| `--skip-tested` | **Reprise intelligente :** Ignore les mod√®les qui poss√®dent d√©j√† des donn√©es de benchmark. Utile pour reprendre un audit interrompu sans tout relancer. |

Exemple :
```bash
python scripts/audit_and_update.py --skip-tested
```

---

## ‚ö†Ô∏è Troubleshooting

**Erreur : `ModuleNotFoundError: No module named 'src'`**
* **Cause :** Vous avez lanc√© le script depuis le dossier `scripts/` (ex: `cd scripts && python setup_models.py`).
* **Solution :** Revenez √† la racine du projet et lancez `python scripts/setup_models.py`.

**Erreur : `Ollama connection failed`**
* **Solution :** Assurez-vous que l'application Ollama tourne en arri√®re-plan et est accessible sur `http://localhost:11434`.

**"La RAM mesur√©e diminue quand le contexte augmente (Swap Detect√©)"**
* **Sympt√¥me :** Le rapport indique une RAM de 0.4GB pour un contexte de 32k tokens, alors qu'elle √©tait de 4GB pour 16k tokens.
* **Cause :** Votre machine a satur√© sa m√©moire physique (RAM). Le syst√®me d'exploitation a d√©plac√© la m√©moire du mod√®le sur le disque dur (**Swapping**). 
* **Cons√©quence :** Le script d√©tecte cette anomalie, arr√™te le test pour ce mod√®le et ne conserve que le "Max Valid Context" (le dernier avant le swap) pour garantir des m√©triques de performance fiables.

**"Mon nouveau mod√®le ajout√© dans JSON n'est pas d√©tect√©"**
* **Sympt√¥me :** Vous avez ajout√© un bloc dans `models.json`, mais `setup_models.py` ne l'installe pas et le script d'audit le saute.
* **Cause :** Il manque probablement la cl√© `"type": "local"` dans votre configuration JSON. Les scripts filtrent les mod√®les pour ignorer ceux bas√©s sur des API Cloud.
* **Solution :** Ajoutez `"type": "local"` dans l'objet JSON du mod√®le.

**"Logs : ‚ö†Ô∏è Bavard (Max output atteint)"**
* **Explication :** Ce n'est pas une erreur. Cela signifie que le mod√®le a g√©n√©r√© une r√©ponse plus longue que la limite de s√©curit√© (512 tokens) impos√©e par le benchmark. Le test est consid√©r√© comme **VALIDE** (la RAM et la vitesse ont bien √©t√© mesur√©es), le script a simplement coup√© la parole au mod√®le pour passer √† la suite.