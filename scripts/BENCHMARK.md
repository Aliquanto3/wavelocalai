# üìä WaveLocalAI - Documentation du Benchmark SLM

## Table des mati√®res

1. [Pourquoi ce benchmark ?](#pourquoi-ce-benchmark-)
2. [Installation et pr√©requis](#installation-et-pr√©requis)
3. [Utilisation](#utilisation)
4. [M√©thodologie des tests](#m√©thodologie-des-tests)
5. [D√©tail des m√©triques mesur√©es](#d√©tail-des-m√©triques-mesur√©es)
6. [Analyse des outputs](#analyse-des-outputs)
7. [Avantages et limites](#avantages-et-limites)
8. [Annexes](#annexes)

---

## Pourquoi ce benchmark ?

### Contexte

L'adoption des Small Language Models (SLM) auto-h√©berg√©s pose des d√©fis sp√©cifiques que les benchmarks acad√©miques traditionnels ne couvrent pas. Ce benchmark se concentre sur **l'op√©rationnel** et la **frugalit√©**.

| Besoin op√©rationnel | Benchmark acad√©mique | Notre benchmark |
|---------------------|---------------------|-----------------|
| Exp√©rience utilisateur (UX) ? | ‚ùå Non mesur√© | ‚úÖ **UX Rating** (Latence per√ßue) |
| Efficience r√©elle (Qualit√©/Co√ªt) ?| ‚ùå Non mesur√© | ‚úÖ **Efficiency Grade** (Raisonnement vs CO‚ÇÇ) |
| Conformit√© juridique ? | ‚ùå Souvent ignor√© | ‚úÖ **D√©tection de Licence** |
| Le mod√®le tiendra-t-il en RAM ? | ‚ùå Non mesur√© | ‚úÖ Mesure par palier avec d√©tection de SWAP |
| Robustesse du contexte ? | ‚ö†Ô∏è "Lost-in-the-middle" ignor√© | ‚úÖ Test Needle multi-positions (10%, 50%, 90%) |

---

## Installation et pr√©requis

### D√©pendances

```bash
# Syst√®me
curl -fsSL [https://ollama.com/install.sh](https://ollama.com/install.sh) | sh
ollama serve

# Python
pip install ollama codecarbon psutil python-dotenv mistralai
```

### Configuration (.env)

```env
# CodeCarbon - Code ISO du pays (Impacte le calcul CO2 selon le mix √©lectrique)
WAVELOCAL_COUNTRY_ISO=FRA
# PUE du datacenter/bureau (Efficacit√© √©nerg√©tique du b√¢timent)
WAVELOCAL_PUE=1.1
# API Mistral (optionnel)
MISTRAL_API_KEY=your_key_here
```

---

## Utilisation

```bash
# Benchmark complet (tous les mod√®les locaux)
python benchmark_slm.py

# Tester des mod√®les sp√©cifiques
python benchmark_slm.py -m qwen2.5:0.5b llama3.2:1b

# Mode "Mise √† jour incr√©mentale" (ne re-teste pas ce qui est fait)
python benchmark_slm.py --skip-tested

# Mode Verbeux (Voir les prompts et r√©ponses compl√®tes pour debug)
python benchmark_slm.py -v
```

---

## M√©thodologie des tests

### 1. Test de mont√©e en contexte
**Protocole** : Envoi de prompts de taille croissante (2K ‚Üí 128K).
**D√©tection Swap** : Si la RAM utilis√©e diminue soudainement (`< 80%` du palier pr√©c√©dent), cela indique que l'OS a d√©charg√© le mod√®le sur le disque (Swap). Le test s'arr√™te pour garantir la fiabilit√©.

### 2. Test Needle-in-Haystack (Robustesse)
**Objectif** : V√©rifier que le mod√®le n'oublie pas d'informations selon leur position dans le contexte.
**Protocole** : Insertion d'un "code secret" √† **10% (d√©but)**, **50% (milieu)**, et **90% (fin)** du contexte. Le mod√®le doit r√©ussir les 3 pour valider le niveau.

### 3. Tests Fonctionnels
- **Multilingue** : Test de compr√©hension et g√©n√©ration avec tol√©rance aux synonymes (11 langues).
- **Tool Calling** : Validation stricte des param√®tres extraits (ex: ville "Paris" bien d√©tect√©e).
- **JSON** : Validation de la conformit√© au sch√©ma (cl√©s requises, types de donn√©es).

---

## D√©tail des m√©triques mesur√©es

Le benchmark g√©n√®re trois types de m√©triques : d√©cisionnelles, techniques et fonctionnelles.

### 1. M√©triques D√©cisionnelles (Strat√©gique)

Ces m√©triques synth√©tiques permettent une prise de d√©cision rapide (Go/No-Go).

| Cl√© JSON | M√©trique | Description & Seuils |
|----------|----------|----------------------|
| `ux_rating` | **Note UX** | Qualifie la fluidit√© bas√©e sur le TTFT (Time To First Token).<br>‚ö° **Instantan√©** : < 300ms<br>üöÄ **Rapide** : < 800ms<br>üê¢ **Acceptable** : < 1500ms<br>üêå **Lent** : > 1500ms |
| `efficiency_grade` | **Efficience** | Ratio entre l'intelligence (Score Raisonnement) et le co√ªt carbone.<br>üü¢ **Excellent** : Mod√®le intelligent et tr√®s l√©ger.<br>üü° **Bon** : Bon compromis.<br>üî¥ **Faible** : Trop √©nergivore pour ses capacit√©s. |
| `detected_license` | **Licence** | D√©tection automatique via m√©tadonn√©es Ollama (ex: `Apache 2.0`, `MIT`, `CC-BY-NC`). Permet de valider l'usage commercial. |

### 2. M√©triques de Performance & Green IT

| Cl√© JSON | M√©trique | Unit√© | Description |
|----------|----------|-------|-------------|
| `avg_tokens_per_second` | Vitesse | tok/s | Vitesse de lecture/g√©n√©ration. >30 est consid√©r√© temps r√©el fluide. |
| `avg_ttft_ms` | Latence | ms | Temps d'attente avant l'affichage du premier caract√®re. |
| `avg_co2_per_1k_tokens` | Empreinte | gCO‚ÇÇ | Grammes de CO‚ÇÇ √©mis pour g√©n√©rer 1000 tokens (environ 750 mots). |
| `ram_usage_at_max_ctx_gb` | M√©moire | GB | RAM r√©elle occup√©e par le mod√®le charg√© au contexte maximum valid√©. |

### 3. M√©triques de Qualit√© (Scores 0-1)

| Cl√© JSON | M√©trique | Description |
|----------|----------|-------------|
| `quality_scores.reasoning_avg` | Raisonnement | % de r√©ussite sur 5 tests de logique (syllogismes, maths simples). |
| `quality_scores.instruction_following_avg` | Suivi | % de r√©ussite sur le respect de consignes de formatage strictes. |
| `tool_capability.success_rate` | Tools | 1.0 si le mod√®le d√©tecte la fonction ET extrait les bons param√®tres. |
| `json_capability.schema_compliance_rate` | JSON | 1.0 si le JSON g√©n√©r√© respecte parfaitement le sch√©ma impos√©. |

---

## Analyse des outputs

### Structure du JSON (`models.json`)

Le fichier `models.json` est la source de v√©rit√©. Voici un exemple complet d'un mod√®le benchmark√© :

```json
"Qwen 2.5 0.5B Instruct": {
    "ollama_tag": "qwen2.5:0.5b",
    "type": "local",
    "benchmark_stats": {
        "date": "2025-12-15",

        // --- Dimensionnement ---
        "max_validated_ctx": 32768,           // Fen√™tre de contexte maximale fiable
        "ram_usage_at_max_ctx_gb": 0.827,     // RAM requise (GB)
        "gpu_vram_usage_gb": 0,               // VRAM utilis√©e (si GPU d√©di√©)

        // --- Performance & UX ---
        "avg_tokens_per_second": 36.08,
        "avg_ttft_ms": 1384,
        "ux_rating": "üê¢ Acceptable",         // <1500ms

        // --- RSE & Conformit√© ---
        "detected_license": "Apache 2.0",     // Usage commercial OK
        "efficiency_grade": "üü¢ Excellent",   // Tr√®s peu de CO2 pour un bon raisonnement
        "total_co2_emissions_kg": 7.88e-05,
        "avg_co2_per_1k_tokens": 5.3e-05,

        // --- Capacit√©s Fonctionnelles ---
        "tool_capability": {
            "function_detection": true,
            "parameter_extraction": true,
            "success_rate": 1.0
        },
        "json_capability": {
            "valid_json_rate": 1.0,
            "schema_compliance_rate": 1.0
        },
        "needle_in_haystack": {               // Test de m√©morisation par palier
            "ctx_4k": true,
            "ctx_8k": true,
            "ctx_16k": true,
            "ctx_32k": false                  // √âchec √† 32k (Lost in the middle ?)
        },
        "quality_scores": {
            "reasoning_avg": 0.8,             // 80% de r√©ussite aux tests logiques
            "instruction_following_avg": 0.5,
            "response_variance_avg": 0.0
        }
    }
}
```

### Indicateurs cl√©s √† surveiller

#### Matrice de D√©cision UX (Bas√©e sur TTFT)
| Grade | Latence (ms) | Ressenti Utilisateur |
|-------|--------------|----------------------|
| ‚ö° Instantan√© | < 300 | Comme une UI native. Id√©al pour auto-compl√©tion. |
| üöÄ Rapide | 300 - 800 | Tr√®s fluide. Id√©al pour chat conversationnel. |
| üê¢ Acceptable | 800 - 1500 | L√©ger d√©lai de r√©flexion perceptible. |
| üêå Lent | > 1500 | L'utilisateur risque de penser que √ßa a plant√©. |

#### Matrice Efficience (Raisonnement / CO‚ÇÇ)
Ce score aide √† choisir le mod√®le le plus "Smart & Green".
- **Calcul** : `(Score Raisonnement * 100) / (Grammes CO‚ÇÇ par 1k tokens)`
- **Interpr√©tation** : Un mod√®le 70B aura un bon raisonnement mais un CO‚ÇÇ √©norme -> Score efficience faible. Un mod√®le 3B bien optimis√© aura un score excellent.

---

## Avantages et limites

### ‚úÖ Avantages
1. **Reproductibilit√©** : Prompts fixes, temp√©rature 0 pour tests fonctionnels.
2. **Vision Holistique** : Combine Technique (RAM), M√©tier (JSON/Tools) et RSE (CO‚ÇÇ).
3. **Op√©rationnel** : Les m√©triques d√©cisionnelles (UX, Licence) permettent un choix rapide.

### ‚ö†Ô∏è Limites
- **CodeCarbon CPU-only** : Sur certaines configurations, seul le CPU est mesur√© par d√©faut.
- **D√©tection Licence** : Bas√©e sur les m√©tadonn√©es d√©claratives du fichier GGUF/Modelfile. Peut √™tre vide.
- **Biais Linguistique** : Les tests de raisonnement sont majoritairement en anglais pour standardiser le score.

---

## Annexes

### Contribuer / Ajouter un mod√®le
Pour ajouter un mod√®le au benchmark, ajoutez son entr√©e dans `models.json` avec son tag Ollama, puis lancez :
```bash
python benchmark_slm.py -m votre-modele:tag
```

*Documentation v2.1 - WaveLocalAI Team*
