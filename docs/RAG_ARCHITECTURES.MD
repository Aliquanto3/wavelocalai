# üß† Documentation Technique : Module RAG WaveLocalAI

Ce document d√©taille l'architecture du module RAG refondu (v2.0), con√ßu pour fonctionner **100% Localement**.

## 1. Architecture Globale

Le syst√®me repose sur une architecture modulaire utilisant le Pattern Strategy.
- **Ingestion** : Support multi-formats (PDF, DOCX, TXT, MD) via `IngestionPipeline`.
- **Stockage** : ChromaDB avec isolation des collections par mod√®le d'embedding.
- **Retrieval** : Strat√©gies interchangeables √† chaud.

## 2. Strat√©gies de Retrieval Impl√©ment√©es

### A. Naive RAG (Baseline)
- **Fonctionnement** : Recherche de similarit√© vectorielle (Cosine Similarity).
- **Option** : Reranking (Cross-Encoder) pour r√©ordonner les r√©sultats.
- **Usage** : Questions factuelles simples, base documentaire petite.
- **Latence** : ‚ö° Faible.

### B. HyDE (Hypothetical Document Embeddings)
- **Fonctionnement** :
    1. Le LLM g√©n√®re une r√©ponse fictive ("Hallucination contr√¥l√©e") √† la question.
    2. Cette r√©ponse est vectoris√©e pour chercher des documents similaires.
    3. Les documents r√©els sont renvoy√©s.
- **Avantage** : Comble le foss√© s√©mantique (question courte vs document technique).
- **Usage** : Questions mal formul√©es ou tr√®s courtes.
- **Latence** : üê¢ Moyenne (+1 appel LLM).

### C. Self-RAG (Corrective RAG)
- **Fonctionnement** (via LangGraph) :
    1. **Retrieve** : R√©cup√®re les documents.
    2. **Grade** : Un LLM Juge √©value la pertinence des docs (Yes/No).
    3. **Decide** :
        - Si pertinent : G√©n√®re la r√©ponse.
        - Si non pertinent : R√©√©crit la question et recommence (Max 2 boucles).
- **Avantage** : R√©duit drastiquement les hallucinations.
- **Usage** : Questions complexes n√©cessitant une haute pr√©cision.
- **Latence** : üê¢üê¢ √âlev√©e (Multiples appels LLM).

## 3. Mod√®les Support√©s (Local)

Les mod√®les sont g√©r√©s dans `data/models/` et charg√©s via `RAGModelsFactory`.

| R√¥le | Mod√®le Recommand√© | Alternative Light |
| :--- | :--- | :--- |
| **Embedding** | `bge-m3` (SOTA, Hybrid) | `all-MiniLM-L6-v2` |
| **Reranker** | `bge-reranker-base` | `mxbai-rerank-base-v1` |
| **LLM Juge** | `Mistral-Nemo` ou `Llama-3` | `Qwen2.5-1.5B` |

## 4. √âvaluation (EvalOps)

Le moteur utilise **Ragas** adapt√© au local.
- **Metrics** : Faithfulness (Fid√©lit√©), Answer Relevancy.
- **Constraint** : L'embedding utilis√© pour l'√©valuation est synchronis√© avec celui du moteur RAG pour garantir la coh√©rence des scores.
