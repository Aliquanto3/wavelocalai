"""
Agent Engine - Moteur d'agent autonome bas√© sur LangGraph.

Modifications principales :
- Support de la s√©lection dynamique d'outils
- Support des mod√®les API (Mistral) en plus d'Ollama
- D√©tection du type de mod√®le via models.json (SOURCE DE V√âRIT√â)
"""

import logging
import os
from collections.abc import Generator

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_ollama import ChatOllama
from langgraph.prebuilt import create_react_agent

from src.core.agent_tools import AVAILABLE_TOOLS, get_tools_by_names
from src.core.model_detector import is_api_model
from src.core.models_db import MODELS_DB, get_model_info

# Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AgentEngine:
    """
    Moteur d'agent autonome bas√© sur LangGraph.
    Capable d'utiliser des outils s√©lectionn√©s pour r√©pondre.
    Supporte les mod√®les locaux (Ollama) et API (Mistral).
    """

    def __init__(self, model_name: str, enabled_tools: list[str] = None):
        """
        Initialise l'agent avec un mod√®le et une liste d'outils.

        Args:
            model_name: Tag du mod√®le (ex: "qwen2.5:1.5b", "mistral-large-2512")
            enabled_tools: Liste des noms d'outils √† activer (None = tous)
                          Ex: ["calculator", "send_email", "system_monitor"]
        """
        self.model_name = model_name

        # 1. S√©lection des outils
        if enabled_tools is None:
            # Par d√©faut, tous les outils sont activ√©s
            self.tools = AVAILABLE_TOOLS
            logger.info(f"Agent initialis√© avec TOUS les outils ({len(AVAILABLE_TOOLS)})")
        else:
            # Filtrage des outils par nom
            self.tools = get_tools_by_names(enabled_tools)
            logger.info(f"Agent initialis√© avec {len(self.tools)} outil(s): {enabled_tools}")

        # 2. D√©tection du type de mod√®le via models.json et initialisation du LLM
        self.llm = self._initialize_llm(model_name)

        # 3. Cr√©ation du graphe d'agent (Prebuilt ReAct Agent)
        self.agent_executor = create_react_agent(self.llm, self.tools)

    def _initialize_llm(self, model_tag: str):
        # Utiliser le d√©tecteur central
        if is_api_model(model_tag):
            return self._initialize_mistral_api(model_tag)
        else:
            return self._initialize_ollama(model_tag)

    def _initialize_mistral_api(self, model_tag: str):
        """
        Initialise un mod√®le Mistral via l'API.

        Args:
            model_tag: Tag du mod√®le
            model_info: Informations du mod√®le depuis models.json

        Returns:
            ChatMistralAI: Instance du mod√®le API
        """
        try:
            from langchain_mistralai import ChatMistralAI

            api_key = os.getenv("MISTRAL_API_KEY")
            if not api_key:
                raise ValueError(
                    f"MISTRAL_API_KEY manquante dans .env pour utiliser le mod√®le API '{model_tag}'. "
                    f"Ajoutez votre cl√© Mistral dans le fichier .env"
                )

            logger.info(f"üåê Initialisation du mod√®le API Mistral : {model_tag} ")

            return ChatMistralAI(
                model=model_tag,
                mistral_api_key=api_key,
                temperature=0.0,  # Z√©ro cr√©ativit√© pour la rigueur des appels d'outils
            )

        except ImportError as e:
            raise ImportError(
                "Le package 'langchain-mistralai' est requis pour les mod√®les Mistral API. "
                "Installez-le avec : pip install langchain-mistralai"
            ) from e

        except Exception as e:
            logger.error(f"Erreur lors de l'initialisation du mod√®le API {model_tag} : {e}")
            raise

    def _initialize_ollama(self, model_tag: str):
        """
        Initialise un mod√®le local via Ollama.

        Args:
            model_tag: Tag du mod√®le

        Returns:
            ChatOllama: Instance du mod√®le local
        """
        logger.info(f"üè† Initialisation du mod√®le local Ollama : {model_tag}")

        return ChatOllama(
            model=model_tag,
            temperature=0.0,  # Z√©ro cr√©ativit√© pour la rigueur des appels d'outils
        )

    def run_stream(
        self,
        user_query: str,
        chat_history: list[dict] | None = None,
        system_prompt: str | None = None,
    ) -> Generator[dict, None, None]:
        """
        Ex√©cute l'agent et stream les √©v√©nements (pens√©es, appels d'outils, r√©ponse finale).

        Args:
            user_query: Question de l'utilisateur
            chat_history: Historique de conversation (optionnel)
            system_prompt: Instructions syst√®me personnalis√©es (optionnel)

        Yields:
            dict: √âv√©nements avec structure {"type": ..., "content": ..., etc.}
        """
        if chat_history is None:
            chat_history = []

        # Valeur par d√©faut si non fournie
        default_system = (
            "Tu es un assistant utile capable d'utiliser des outils. "
            "Si tu utilises un outil, base ta r√©ponse finale sur son r√©sultat. "
            "R√©ponds dans la m√™me langue que l'utilisateur."
        )
        final_system_prompt = system_prompt if system_prompt else default_system

        # Construction des messages LangChain
        lc_messages = [SystemMessage(content=final_system_prompt)]

        for msg in chat_history:
            if msg["role"] == "user":
                lc_messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                lc_messages.append(AIMessage(content=msg["content"]))

        lc_messages.append(HumanMessage(content=user_query))

        try:
            # Stream des √©v√©nements du graphe
            stream = self.agent_executor.stream({"messages": lc_messages}, stream_mode="values")

            seen_messages = set()

            for event in stream:
                messages = event.get("messages", [])
                if not messages:
                    continue

                last_message = messages[-1]
                msg_id = id(last_message)

                if msg_id in seen_messages:
                    continue
                seen_messages.add(msg_id)

                # A. Appel d'outil (Tool Call)
                if isinstance(last_message, AIMessage) and last_message.tool_calls:
                    for tool_call in last_message.tool_calls:
                        yield {
                            "type": "tool_call",
                            "tool": tool_call["name"],
                            "args": tool_call["args"],
                        }

                # B. R√©sultat d'outil (Tool Message)
                elif hasattr(last_message, "tool_call_id"):
                    yield {"type": "tool_result", "content": last_message.content}

                # C. R√©ponse Finale (AIMessage sans tool_calls)
                elif isinstance(last_message, AIMessage) and not last_message.tool_calls:
                    yield {"type": "final_answer", "content": last_message.content}

        except Exception as e:
            logger.error(f"Erreur Agent: {e}")
            yield {"type": "error", "content": f"Erreur critique de l'agent : {str(e)}"}


# ========================================
# UTILITAIRES POUR TESTS & DEBUGGING
# ========================================


def list_available_models():
    """Liste tous les mod√®les disponibles avec leur type."""
    if not MODELS_DB:
        print("‚ùå Base de donn√©es des mod√®les vide")
        return

    print(f"\nüìã {len(MODELS_DB)} mod√®les disponibles :\n")

    for model_name, info in MODELS_DB.items():
        model_type = info.get("type", "unknown")
        ollama_tag = info.get("ollama_tag", "N/A")
        icon = "üåê" if model_type == "api" else "üè†"

        print(f"{icon} {model_name:<40} | Tag: {ollama_tag:<25} | Type: {model_type}")


def test_model_detection(model_tag: str):
    """Teste la d√©tection du type d'un mod√®le."""
    print(f"\nüîç Test de d√©tection pour : {model_tag}\n")

    info = get_model_info(model_tag)

    if info:
        print("‚úÖ Mod√®le trouv√© dans models.json")
        print(f"   Type : {info.get('type', 'N/A')}")
        print(f"   √âditeur : {info.get('editor', 'N/A')}")
        print(f"   Param√®tres : {info.get('params_tot', 'N/A')}")
        print(f"   Capacit√©s : {', '.join(info.get('capabilities', []))}")
    else:
        print("‚ùå Mod√®le non trouv√© dans models.json")
        print("   Fallback : Utilisation d'Ollama par d√©faut")


if __name__ == "__main__":
    # Tests de base
    print("=" * 80)
    print("AGENT ENGINE - TESTS DE D√âTECTION DE MOD√àLES")
    print("=" * 80)

    # Liste tous les mod√®les
    list_available_models()

    # Tests de d√©tection
    print("\n" + "=" * 80)
    print("TESTS DE D√âTECTION")
    print("=" * 80)

    test_cases = [
        "qwen2.5:1.5b",  # Local
        "mistral-large-2512",  # API
        "devstral-2512",  # API
        "mistral:7b",  # Local (Mistral via Ollama)
        "model-inconnu",  # Non trouv√©
    ]

    for model_tag in test_cases:
        test_model_detection(model_tag)
