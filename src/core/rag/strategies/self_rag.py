import asyncio
import logging
from typing import Any, TypedDict

from langchain_core.documents import Document

# LangGraph & LangChain
from langgraph.graph import END, StateGraph

from src.core.llm_provider import LLMProvider

# Core App
from src.core.rag.strategies.base import RetrievalStrategy

logger = logging.getLogger(__name__)


# --- 1. D√âFINITION DE L'√âTAT DU GRAPHE ---
class GraphState(TypedDict):
    """
    L'√©tat qui circule dans le graphe.
    """

    question: str  # La question (originale ou r√©√©crite)
    original_question: str  # Pour r√©f√©rence
    documents: list[Document]  # Les docs r√©cup√©r√©s
    generation: str  # La r√©ponse finale
    loop_step: int  # Compteur pour √©viter les boucles infinies


class SelfRAGStrategy(RetrievalStrategy):
    """
    Strat√©gie Self-RAG (Corrective RAG) locale avec LangGraph.
    V√©rifie la pertinence des documents avant de r√©pondre.
    """

    def __init__(self, grader_llm: str = "qwen2.5:1.5b"):
        # On utilise un "petit" mod√®le rapide pour la notation (Grading)
        self.grader_llm_tag = grader_llm
        self.MAX_LOOPS = 2  # S√©curit√© : max 2 r√©√©critures

    # --- 2. LES NOEUDS (NODES) ---

    async def retrieve_node(self, state: GraphState, vector_store, k, reranker):
        """Node: R√©cup√®re les documents."""
        logger.info(f"üîÑ [Self-RAG] Retrieval pour : {state['question']}")

        # On utilise la logique standard (Similarity + Rerank)
        # Note: On duplique un peu la logique 'Naive' ici pour l'int√©grer au graphe
        docs = vector_store.similarity_search(state["question"], k=k * 2)  # Fetch large

        if reranker and docs:
            try:
                pairs = [[state["question"], doc.page_content] for doc in docs]
                scores = reranker.predict(pairs)
                scored_docs = sorted(
                    zip(docs, scores, strict=False), key=lambda x: x[1], reverse=True
                )
                docs = [d for d, s in scored_docs[:k]]
            except Exception:
                docs = docs[:k]
        else:
            docs = docs[:k]

        return {"documents": docs, "question": state["question"]}

    async def grade_documents_node(self, state: GraphState):
        """Node: Filtre les documents non pertinents."""
        logger.info("‚öñÔ∏è [Self-RAG] Grading documents...")
        question = state["question"]
        documents = state["documents"]

        filtered_docs = []

        # Prompt de notation binaire (JSON mode implicite)
        system_prompt = (
            "You are a grader assessing relevance of a retrieved document to a user question. "
            "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. "
            "Reply only with 'yes' or 'no'."
        )

        for doc in documents:
            # Appel LLM l√©ger pour chaque doc (parall√©lisable id√©alement)
            messages = [
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": f"Document: {doc.page_content[:400]}...\n\nQuestion: {question}",
                },
            ]

            # On utilise chat_stream en mode "one shot"
            response_text = ""
            async for chunk in LLMProvider.chat_stream(
                self.grader_llm_tag, messages, temperature=0
            ):
                if isinstance(chunk, str):
                    response_text += chunk

            grade = response_text.strip().lower()

            if "yes" in grade:
                filtered_docs.append(doc)
            else:
                logger.debug("   ‚ùå Doc rejet√©")

        return {"documents": filtered_docs}

    async def rewrite_node(self, state: GraphState):
        """Node: R√©√©crit la question pour am√©liorer le retrieval."""
        logger.info("‚úçÔ∏è [Self-RAG] R√©√©criture de la question...")
        question = state["question"]

        msg = [
            {
                "role": "system",
                "content": "You are a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval. Look at the initial and formulate an improved question. Just output the question.",
            },
            {"role": "user", "content": f"Initial question: {question}"},
        ]

        better_question = ""
        async for chunk in LLMProvider.chat_stream(self.grader_llm_tag, msg, temperature=0.5):
            if isinstance(chunk, str):
                better_question += chunk

        return {"question": better_question, "loop_step": state["loop_step"] + 1}

    # --- 3. LES BORDS (EDGES) ---

    def decide_to_generate(self, state: GraphState):
        """Edge Conditionnel : G√©n√©rer ou R√©√©crire ?"""
        filtered_documents = state["documents"]
        loop_step = state.get("loop_step", 0)

        if not filtered_documents:
            # Aucun doc pertinent trouv√©
            if loop_step >= self.MAX_LOOPS:
                logger.warning("üõë [Self-RAG] Max loops reached. Stop.")
                return "stop_empty"  # Cas d'abandon
            else:
                logger.info("üîÑ [Self-RAG] Documents insuffisants -> Rewrite.")
                return "rewrite"
        else:
            # On a des docs pertinents
            return "generate"

    # --- 4. MAIN EXECUTION ---

    def retrieve(
        self, query: str, vector_store: Any, k: int, reranker: Any = None, **kwargs
    ) -> list[Document]:

        # Construction du graphe (√† chaque appel pour simplifier le passage des objets k/store)
        workflow = StateGraph(GraphState)

        # D√©finition des noeuds (wrappers asynchrones n√©cessaires pour LangGraph)
        # Note: Dans une app pure LangGraph, on compilerait le graphe une seule fois.
        # Ici, on l'utilise de mani√®re ad-hoc pour s'int√©grer dans ta structure de classe.

        async def _retrieve_wrapper(state):
            return await self.retrieve_node(state, vector_store, k, reranker)

        workflow.add_node("retrieve", _retrieve_wrapper)
        workflow.add_node("grade_documents", self.grade_documents_node)
        workflow.add_node("rewrite", self.rewrite_node)

        # Construction des arcs
        workflow.set_entry_point("retrieve")
        workflow.add_edge("retrieve", "grade_documents")

        workflow.add_conditional_edges(
            "grade_documents",
            self.decide_to_generate,
            {
                "rewrite": "rewrite",
                "generate": END,  # Succ√®s : on retourne les docs filtr√©s
                "stop_empty": END,  # √âchec : on retourne liste vide (ou docs originaux selon choix)
            },
        )
        workflow.add_edge("rewrite", "retrieve")

        app = workflow.compile()

        # Ex√©cution du graphe
        logger.info(f"üöÄ D√©marrage Self-RAG Graph pour : {query}")

        # On doit utiliser asyncio.run car retrieve est sync dans la classe de base
        # mais LangGraph est async.
        try:
            inputs = {"question": query, "original_question": query, "loop_step": 0}

            # Invocation
            final_state = asyncio.run(app.ainvoke(inputs))

            final_docs = final_state.get("documents", [])
            logger.info(f"üèÅ Fin Self-RAG. Docs retenus : {len(final_docs)}")

            # Marquage des m√©tadonn√©es pour l'UI
            for doc in final_docs:
                doc.metadata["strategy"] = "Self-RAG"
                doc.metadata["final_query"] = final_state["question"]  # Pour voir si r√©√©criture

            return final_docs

        except Exception as e:
            logger.error(f"‚ùå Erreur Critical Self-RAG: {e}")
            # Fallback Naive en cas de crash du graphe
            return vector_store.similarity_search(query, k=k)
