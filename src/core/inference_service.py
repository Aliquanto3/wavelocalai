"""
Service d'orchestration d'infÃ©rence dÃ©couplÃ© de l'UI.
Permet la rÃ©utilisation pour benchmarks, agents et Ã©valuations.
"""
import asyncio
import logging
from collections.abc import Awaitable, Callable
from dataclasses import dataclass, field
from datetime import datetime
from typing import Optional

from src.core.llm_provider import LLMProvider
from src.core.metrics import InferenceMetrics
from src.core.models_db import extract_thought

# Logging
logger = logging.getLogger(__name__)


# ========================================
# 1. STRUCTURES DE DONNÃ‰ES
# ========================================


@dataclass
class InferenceResult:
    """RÃ©sultat complet d'une infÃ©rence."""

    raw_text: str
    clean_text: str
    thought: Optional[str]
    metrics: Optional[InferenceMetrics]
    error: Optional[str] = None
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class InferenceCallbacks:
    """
    Callbacks optionnels pour feedback temps rÃ©el.
    Permet au frontend de s'abonner aux Ã©vÃ©nements sans couplage.
    """

    on_token: Optional[Callable[[str], Awaitable[None]]] = None
    on_metrics: Optional[Callable[[InferenceMetrics], Awaitable[None]]] = None
    on_thought: Optional[Callable[[str], Awaitable[None]]] = None
    on_error: Optional[Callable[[str], Awaitable[None]]] = None


# ========================================
# 2. SERVICE D'INFÃ‰RENCE
# ========================================


class InferenceService:
    """
    Orchestrateur pur (sans dÃ©pendance UI) pour exÃ©cution d'infÃ©rences.

    Usage:
        # Sans callbacks (mode batch)
        result = await InferenceService.run_inference(model, messages)

        # Avec callbacks (mode streaming UI)
        callbacks = InferenceCallbacks(on_token=update_ui)
        result = await InferenceService.run_inference(model, messages, callbacks)
    """

    @staticmethod
    async def run_inference(
        model_tag: str,
        messages: list[dict[str, str]],
        temperature: float = 0.7,
        system_prompt: Optional[str] = None,
        callbacks: Optional[InferenceCallbacks] = None,
        timeout: int = 120,
    ) -> InferenceResult:
        """
        ExÃ©cute une infÃ©rence avec gestion complÃ¨te des Ã©vÃ©nements.

        Args:
            model_tag: Tag Ollama du modÃ¨le (ex: "qwen2.5:1.5b")
            messages: Historique de conversation [{"role": "user", "content": "..."}]
            temperature: CrÃ©ativitÃ© du modÃ¨le (0.0 = dÃ©terministe, 1.0 = crÃ©atif)
            system_prompt: Instruction systÃ¨me optionnelle
            callbacks: Gestionnaires d'Ã©vÃ©nements optionnels
            timeout: Timeout en secondes (dÃ©faut: 2 minutes)

        Returns:
            InferenceResult contenant texte, pensÃ©e et mÃ©triques

        Raises:
            asyncio.TimeoutError: Si l'infÃ©rence dÃ©passe le timeout
            Exception: Erreurs Ollama ou rÃ©seau
        """
        try:
            # Protection timeout
            return await asyncio.wait_for(
                InferenceService._execute_inference(
                    model_tag, messages, temperature, system_prompt, callbacks
                ),
                timeout=timeout,
            )

        except asyncio.TimeoutError:
            error_msg = f"Timeout ({timeout}s) dÃ©passÃ© pour {model_tag}"
            logger.error(error_msg)
            if callbacks and callbacks.on_error:
                await callbacks.on_error(error_msg)
            return InferenceResult(
                raw_text="", clean_text="", thought=None, metrics=None, error=error_msg
            )

        except Exception as e:
            error_msg = f"Erreur infÃ©rence {model_tag}: {e}"
            logger.exception(error_msg)
            if callbacks and callbacks.on_error:
                await callbacks.on_error(str(e))
            return InferenceResult(
                raw_text="", clean_text="", thought=None, metrics=None, error=str(e)
            )

    @staticmethod
    async def _execute_inference(
        model_tag: str,
        messages: list[dict[str, str]],
        temperature: float,
        system_prompt: Optional[str],
        callbacks: Optional[InferenceCallbacks],
    ) -> InferenceResult:
        """Logique d'exÃ©cution interne (sans timeout wrapper)."""

        full_text = ""
        final_metrics = None

        # Appel du provider
        stream = LLMProvider.chat_stream(
            model_name=model_tag,
            messages=messages,
            temperature=temperature,
            system_prompt=system_prompt,
        )

        # Consommation du stream
        async for item in stream:
            if isinstance(item, str):
                full_text += item
                # Callback token
                if callbacks and callbacks.on_token:
                    await callbacks.on_token(item)

            elif isinstance(item, InferenceMetrics):
                final_metrics = item
                # Callback mÃ©triques
                if callbacks and callbacks.on_metrics:
                    await callbacks.on_metrics(item)

        # Extraction de la pensÃ©e (Chain of Thought)
        thought, clean_text = extract_thought(full_text)

        # Callback pensÃ©e (si dÃ©tectÃ©e)
        if thought and callbacks and callbacks.on_thought:
            await callbacks.on_thought(thought)

        return InferenceResult(
            raw_text=full_text,
            clean_text=clean_text or full_text,  # Fallback si pas de <think>
            thought=thought,
            metrics=final_metrics,
        )

    @staticmethod
    async def run_batch_inference(
        model_tag: str, prompts: list[str], temperature: float = 0.7, max_concurrent: int = 3
    ) -> list[InferenceResult]:
        """
        ExÃ©cute plusieurs infÃ©rences en parallÃ¨le (pour benchmarks).

        Args:
            model_tag: ModÃ¨le Ã  utiliser
            prompts: Liste de prompts Ã  traiter
            temperature: TempÃ©rature d'infÃ©rence
            max_concurrent: Nombre d'infÃ©rences parallÃ¨les max

        Returns:
            Liste de rÃ©sultats dans l'ordre des prompts
        """
        semaphore = asyncio.Semaphore(max_concurrent)

        async def _run_with_semaphore(prompt: str) -> InferenceResult:
            async with semaphore:
                messages = [{"role": "user", "content": prompt}]
                return await InferenceService.run_inference(model_tag, messages, temperature)

        tasks = [_run_with_semaphore(p) for p in prompts]
        return await asyncio.gather(*tasks, return_exceptions=False)


# ========================================
# 3. EXEMPLE D'USAGE (Tests)
# ========================================

if __name__ == "__main__":
    """Tests rapides du service."""

    async def test_simple():
        """Test basique sans callbacks."""
        print("ðŸ§ª Test 1: InfÃ©rence simple")
        result = await InferenceService.run_inference(
            model_tag="qwen2.5:1.5b",
            messages=[{"role": "user", "content": "Dis bonjour en 3 mots"}],
            temperature=0.0,
        )
        print(f"âœ… RÃ©sultat: {result.clean_text}")
        print(f"ðŸ“Š Tokens/s: {result.metrics.tokens_per_second if result.metrics else 'N/A'}")

    async def test_avec_callbacks():
        """Test avec callbacks (simulation UI)."""
        print("\nðŸ§ª Test 2: Avec callbacks")

        current_text = ""

        async def on_token(token: str):
            nonlocal current_text
            current_text += token
            print(f"\rðŸ’¬ Streaming: {current_text[:50]}...", end="", flush=True)

        async def on_metrics(m: InferenceMetrics):
            print(f"\nðŸ“ˆ Vitesse: {m.tokens_per_second} t/s")

        callbacks = InferenceCallbacks(on_token=on_token, on_metrics=on_metrics)

        result = await InferenceService.run_inference(
            model_tag="qwen2.5:1.5b",
            messages=[{"role": "user", "content": "Explique la photosynthÃ¨se en 50 mots"}],
            callbacks=callbacks,
        )
        print(f"\nâœ… Texte final: {result.clean_text}")

    async def test_batch():
        """Test batch (benchmarks)."""
        print("\nðŸ§ª Test 3: Batch de 3 prompts")

        prompts = ["Capitale de la France ?", "2 + 2 = ?", "Quelle est la couleur du ciel ?"]

        results = await InferenceService.run_batch_inference(
            model_tag="qwen2.5:1.5b", prompts=prompts, max_concurrent=2
        )

        for i, result in enumerate(results):
            print(f"âœ… Prompt {i+1}: {result.clean_text[:30]}...")

    # ExÃ©cution des tests
    async def main():
        await test_simple()
        await test_avec_callbacks()
        await test_batch()

    asyncio.run(main())
