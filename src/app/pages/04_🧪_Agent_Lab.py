import streamlit as st
import time
from src.core.llm_provider import LLMProvider
from src.core.agent_engine import AgentEngine
from src.core.models_db import MODELS_DB, get_friendly_name_from_tag, get_model_info, extract_thought

st.set_page_config(page_title="Agent Lab", page_icon="ü§ñ", layout="wide")

st.title("ü§ñ Agent Lab (LangGraph)")
st.caption("Observez une IA utiliser des outils pour r√©soudre des t√¢ches complexes.")

# --- SESSION ---
if "agent_messages" not in st.session_state: st.session_state.agent_messages = []

# --- SIDEBAR ---
with st.sidebar:
    st.header("Configuration de l'Agent")
    
    # 1. LISTING ET TRI INTELLIGENT
    installed = LLMProvider.list_models()
    
    verified_models = []
    experimental_models = []
    
    for m in installed:
        friendly = get_friendly_name_from_tag(m['model'])
        info = get_model_info(friendly)
        
        if info:
            # Cas A : Mod√®le connu dans la DB
            if "tools" in info.get("capabilities", []):
                verified_models.append(friendly)
            # Sinon (connu mais pas compatible), on l'ignore pour √©viter le crash s√ªr
        else:
            # Cas B : Mod√®le manuel (Inconnu de la DB)
            # On l'accepte mais on le marque comme exp√©rimental
            experimental_models.append(friendly)
    
    # Fusion des listes
    sorted_verified = sorted(verified_models)
    sorted_experimental = sorted(experimental_models)
    
    # On pr√©pare les options avec des s√©parateurs visuels si besoin
    # Mais le plus simple est une liste unique avec des pr√©fixes ou juste m√©lang√©e
    all_choices = sorted_verified + sorted_experimental
    
    if not all_choices:
        st.error("Aucun mod√®le trouv√©.")
        st.stop()
        
    # Logique d'affichage dans le selectbox
    def format_func(option):
        if option in sorted_experimental:
            return f"‚ö†Ô∏è {option} (Non v√©rifi√©)"
        return f"‚úÖ {option}"

    selected_friendly = st.selectbox(
        "Cerveau (LLM)", 
        all_choices, 
        format_func=format_func, # Affiche les emojis
        help="Les mod√®les ‚úÖ sont valid√©s pour les outils. Les mod√®les ‚ö†Ô∏è peuvent √©chouer (Erreur 400)."
    )
    
    # R√©cup√©ration du tag technique
    info = get_model_info(selected_friendly)
    if info:
        selected_tag = info['ollama_tag']
    else:
        # Pour les mod√®les exp√©rimentaux, le friendly name EST souvent le tag ou proche
        # On doit retrouver le tag original depuis la liste 'installed'
        # C'est un peu trickier car get_friendly_name_from_tag a transform√© le nom
        # On refait une passe inverse rapide :
        for m in installed:
            if get_friendly_name_from_tag(m['model']) == selected_friendly:
                selected_tag = m['model']
                break

    st.markdown("---")
    st.info(
        """
        **Outils disponibles :**
        1. üïí **Time :** Heure actuelle syst√®me.
        2. üßÆ **Calculator :** Calculs math√©matiques.
        3. üè¢ **Wavestone Search :** Base de connaissances interne.
        """
    )
    
    if st.button("üóëÔ∏è Reset M√©moire"):
        st.session_state.agent_messages = []
        st.rerun()

# --- CHAT INTERFACE ---
for msg in st.session_state.agent_messages:
    with st.chat_message(msg["role"]):
        # Affichage logs outils
        if msg.get("type") == "tool_log":
            with st.status(f"üõ†Ô∏è Utilisation : {msg['tool']}", state="complete"):
                st.write(f"**Args :** `{msg['args']}`")
                st.write(f"**R√©sultat :** {msg['content']}")
        
        # Affichage pens√©e cach√©e (si sauvegard√©e)
        elif msg.get("thought"):
            with st.expander("üí≠ Raisonnement du mod√®le (Interne)", expanded=False):
                st.markdown(msg["thought"])
            st.markdown(msg["content"])
            
        else:
            st.markdown(msg["content"])

if prompt := st.chat_input("Ex: 'Quelle heure est-il et combien font 45 fois 12 ?'"):
    if not selected_tag:
        st.error("Veuillez s√©lectionner un mod√®le compatible.")
        st.stop()

    # 1. User Msg
    st.session_state.agent_messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # 2. Agent Loop
    with st.chat_message("assistant"):
        container = st.container()
        engine = AgentEngine(selected_tag)
        
        full_response = ""
        thought_content = None
        
        # --- METRIQUES ---
        start_time = time.perf_counter()
        last_step_time = start_time
        
        # On it√®re sur le stream
        stream = engine.run_stream(prompt, st.session_state.agent_messages)
        
        try:
            for event in stream:
                current_time = time.perf_counter()
                step_duration = current_time - last_step_time
                last_step_time = current_time
                
                event_type = event["type"]
                
                if event_type == "tool_call":
                    with container.status(f"üî® L'agent r√©fl√©chit... ({step_duration:.2f}s)", expanded=True) as status:
                        st.write(f"**Outil choisi :** `{event['tool']}`")
                        st.write(f"**Arguments :** `{event['args']}`")
                        status.update(label=f"üî® Appel outil : {event['tool']}", state="running")
                        
                        # Log historique
                        st.session_state.agent_messages.append({
                            "role": "assistant",
                            "type": "tool_log",
                            "tool": event['tool'],
                            "args": event['args'],
                            "content": "..." 
                        })
                        
                elif event_type == "tool_result":
                    # Update historique pr√©c√©dent
                    if st.session_state.agent_messages and st.session_state.agent_messages[-1].get("type") == "tool_log":
                        st.session_state.agent_messages[-1]["content"] = event["content"]
                        container.info(f"‚úÖ R√©sultat ({step_duration:.2f}s) : {event['content']}")

                elif event_type == "final_answer":
                    raw_content = event["content"]
                    
                    # Extraction du Raisonnement (<think>)
                    thought_content, clean_response = extract_thought(raw_content)
                    full_response = clean_response
                    
                    # Affichage
                    if thought_content:
                        with container.expander("üí≠ Raisonnement du mod√®le (Chain of Thought)", expanded=True):
                            st.markdown(thought_content)
                    
                    container.markdown(full_response)
                
                elif event_type == "error":
                    container.error(event["content"])

            # Fin ex√©cution
            total_duration = time.perf_counter() - start_time
            container.caption(f"üèÅ T√¢che termin√©e en {total_duration:.2f}s")
            
            # Sauvegarde r√©ponse finale
            if full_response:
                st.session_state.agent_messages.append({
                    "role": "assistant", 
                    "content": full_response,
                    "thought": thought_content # On sauvegarde la pens√©e aussi
                })
                
        except Exception as e:
            container.error(f"Erreur d'ex√©cution : {e}")