import asyncio
import logging

import nest_asyncio
import streamlit as st

from src.core.eval_engine import EvalEngine
from src.core.llm_provider import LLMProvider
from src.core.models_db import get_friendly_name_from_tag
from src.core.resource_manager import ResourceManager

# --- PATCH ASYNCIO (CRITIQUE POUR RAGAS DANS STREAMLIT) ---
nest_asyncio.apply()

# Config Logging Page
logger = logging.getLogger(__name__)

st.set_page_config(page_title="Eval Ops & Quality", page_icon="üéØ", layout="wide")
# ... le reste du fichier reste IDENTIQUE, copie-le depuis ta version actuelle ...
st.title("üéØ Eval Ops : RAG Quality Audit")
st.caption("√âvaluation 'LLM-as-a-Judge' utilisant Mistral AI pour noter vos mod√®les locaux.")

# --- INITIALISATION LAZY ---
if "rag_engine" not in st.session_state:
    st.warning(
        "‚ö†Ô∏è Le moteur RAG n'est pas initialis√©. Veuillez passer par la page '03 RAG Knowledge' d'abord."
    )
    st.stop()

if "eval_engine" not in st.session_state:
    try:
        st.session_state.eval_engine = EvalEngine()
    except ValueError as e:
        st.error(f"‚ùå Configuration manquante : {e}")
        st.info("Ajoutez MISTRAL_API_KEY dans votre fichier .env")
        st.stop()
    except ImportError:
        st.error("‚ùå Librairie 'ragas' manquante. Installez-la via requirements.txt")
        st.stop()

# --- CONFIGURATION DU TEST ---
col_conf, col_run = st.columns([1, 2])

with col_conf:
    st.subheader("1. Candidat (Local)")

    # S√©lection du mod√®le √âl√®ve
    installed = LLMProvider.list_models()
    local_models = [m for m in installed if m.get("type") == "local"]
    model_map = {get_friendly_name_from_tag(m["model"]): m["model"] for m in local_models}

    selected_friendly = st.selectbox("Mod√®le √† √©valuer", sorted(model_map.keys()))
    candidate_tag = model_map.get(selected_friendly)

    # Affichage info RAM (Resource Manager UI)
    if candidate_tag:
        ram_check = ResourceManager.check_resources(candidate_tag)
        if ram_check.allowed:
            st.success(
                f"RAM Dispo : {ram_check.ram_available_gb:.1f} GB (Besoin ~{ram_check.ram_required_gb:.1f} GB)"
            )
        else:
            st.error(f"‚ö†Ô∏è {ram_check.message}")
            st.stop()

    st.markdown("---")
    st.subheader("2. Juge (Cloud)")
    st.info("ü§ñ **Mistral Large** (API)")
    st.caption("Le juge utilise l'API pour ne pas surcharger la RAM locale.")

with col_run:
    st.subheader("3. Protocole de Test")

    query = st.text_area(
        "Question de test",
        "Quelle est la politique de confidentialit√© du projet WaveLocalAI ?",
        help="Posez une question dont la r√©ponse se trouve dans vos documents RAG.",
    )

    if st.button("üöÄ Lancer l'Audit Qualit√©", type="primary"):
        if not candidate_tag:
            st.error("S√©lectionnez un mod√®le.")
            st.stop()

        status = st.status("audit en cours...", expanded=True)

        try:
            # √âTAPE A : RAG RETRIEVAL
            status.write("üîç 1. Recherche des contextes (Vector Store)...")
            retrieved_docs = st.session_state.rag_engine.search(query, k=3)
            contexts = [doc.page_content for doc in retrieved_docs]

            if not contexts:
                status.update(label="‚ùå Erreur : Aucun contexte trouv√© !", state="error")
                st.stop()

            status.write(f"   ‚úÖ {len(contexts)} chunks r√©cup√©r√©s.")

            # √âTAPE B : GENERATION CANDIDAT
            status.write(f"üé§ 2. G√©n√©ration de la r√©ponse par {selected_friendly}...")

            # Construction prompt simple pour le test
            prompt_rag = f"Contexte:\n{chr(10).join(contexts)}\n\nQuestion: {query}\nR√©ponse:"
            messages = [{"role": "user", "content": prompt_rag}]

            # Appel via InferenceService ou LLMProvider directement (ici Provider pour simplicit√©)
            # On utilise un wrapper async pour appeler le provider
            async def get_response():
                resp_text = ""
                stream = LLMProvider.chat_stream(candidate_tag, messages, temperature=0.1)
                async for chunk in stream:
                    if isinstance(chunk, str):
                        resp_text += chunk
                return resp_text

            generated_answer = asyncio.run(get_response())
            status.write("   ‚úÖ R√©ponse g√©n√©r√©e.")

            # √âTAPE C : JUGEMENT (RAGAS)
            status.write("‚öñÔ∏è 3. D√©lib√©ration du Juge (Mistral Large)...")
            eval_result = st.session_state.eval_engine.evaluate_single_turn(
                query=query, response=generated_answer, retrieved_contexts=contexts
            )

            status.update(label="‚úÖ Audit Termin√© !", state="complete", expanded=False)

            # --- R√âSULTATS ---
            st.divider()

            # 1. Scorecard (Mise √† jour pour 2 m√©triques)
            c1, c2, c3 = st.columns(3)
            c1.metric("Note Globale", f"{eval_result.global_score * 100:.0f}/100")
            c2.metric(
                "Fid√©lit√© (Faithfulness)",
                f"{eval_result.faithfulness:.2f}",
                help="La r√©ponse respecte-t-elle strictement le contexte ?",
            )
            c3.metric(
                "Pertinence R√©ponse",
                f"{eval_result.answer_relevancy:.2f}",
                help="La r√©ponse adresse-t-elle la question ?",
            )

            # 2. D√©tails
            col_res, col_ctx = st.columns(2)

            with col_res:
                st.markdown("### ü§ñ R√©ponse du Candidat")
                st.info(generated_answer)

            with col_ctx:
                st.markdown("### üìÑ Contextes Soumis")
                for i, ctx in enumerate(contexts):
                    with st.expander(f"Source {i+1}", expanded=False):
                        st.caption(ctx)

        except Exception as e:
            status.update(label="‚ùå Erreur Critique", state="error")
            st.error(f"D√©tail de l'erreur : {str(e)}")
            logger.exception("EvalOps Failure")
