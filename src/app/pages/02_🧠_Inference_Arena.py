import streamlit as st
import pandas as pd
import time
import asyncio
from src.core.llm_provider import LLMProvider
from src.core.metrics import InferenceMetrics
from src.core.models_db import MODELS_DB, get_model_info, get_all_friendly_names, get_all_languages, get_friendly_name_from_tag, extract_thought

st.set_page_config(page_title="Inf√©rence & Arena", page_icon="üß†", layout="wide")

# --- CSS Custom pour les m√©triques ---
st.markdown("""
<style>
    [data-testid="stMetricValue"] { font-size: 24px; }
    .stTextArea textarea { font-family: monospace; }
</style>
""", unsafe_allow_html=True)

st.title("üß† Inf√©rence & Model Arena")
st.caption("Benchmark technique et fonctionnel des SLM.")

# --- DATA: BIBLIOTH√àQUE DE CAS D'USAGE (PROMPTS) ---
USE_CASES = {
    "üá¨üáß Traduction Technique": {
        "system": "Tu es un expert en traduction technique. Traduis le texte suivant en Anglais, Espagnol et Allemand. Sois pr√©cis sur la terminologie informatique. R√©ponds au format JSON : {\"en\": \"...\", \"es\": \"...\", \"de\": \"...\"}.",
        "user": "L'architecture 'Local First' permet de r√©duire la latence r√©seau et d'am√©liorer la confidentialit√© des donn√©es en traitant les inf√©rences directement sur le CPU de l'utilisateur, sans appel API vers le cloud."
    },
    "üìÑ Extraction Structur√©e (JSON)": {
        "system": "Tu es un extracteur de donn√©es strict. Extrais les entit√©s du texte (Date, Montant, Vendeur, Articles). R√©ponds UNIQUEMENT avec un JSON valide. Pas de texte avant ni apr√®s.",
        "user": "FACTURE N¬∞ 2024-001\nDate : 12 d√©cembre 2024\nVendeur : Wavestone Tech\n\nArticles :\n- 1x Audit Green IT (500‚Ç¨)\n- 3x Licences Copilot (90‚Ç¨)\n\nTotal TTC : 590‚Ç¨"
    },
    "üíª Assistant Coding (Python)": {
        "system": "Tu es un Tech Lead Python exp√©riment√©. G√©n√®re du code propre, typ√© (Type Hints) et document√© (Docstrings). Inclus une gestion d'erreur robuste.",
        "user": "√âcris une fonction Python asynchrone qui interroge une API REST avec la librairie 'httpx', g√®re les retries en cas d'erreur 500, et retourne le r√©sultat en dictionnaire."
    },
    "üßÆ Raisonnement (Chain of Thought)": {
        "system": "Tu es un expert en logique. Pour r√©pondre, tu dois IMP√âRATIVEMENT utiliser la m√©thode 'Chain of Thought' : explique ton raisonnement √©tape par √©tape avant de donner la r√©ponse finale.",
        "user": "J'ai 3 pommes. Hier j'en ai mang√© une. Aujourd'hui j'en ach√®te deux autres, mais j'en fais tomber une dans la boue que je jette. Combien de pommes puis-je manger maintenant ?"
    },
    "üìù R√©sum√© Ex√©cutif": {
        "system": "Tu es un assistant de direction. Fais un r√©sum√© concis (bullet points) du texte fourni, en te concentrant sur les d√©cisions cl√©s et les actions √† entreprendre.",
        "user": "Compte rendu de r√©union - Projet Alpha.\nLa r√©union a d√©but√© √† 10h. L'√©quipe a convenu que le budget initial √©tait insuffisant. Marc doit revoir le fichier Excel d'ici mardi. Sophie a soulev√© un risque de s√©curit√© sur l'API, il faut auditer le module d'auth. La deadline du projet est repouss√©e de 2 semaines pour permettre ces ajustements. Le client a valid√© le nouveau design."
    }
}

# --- SESSION STATE ---
if "messages" not in st.session_state: st.session_state.messages = []
if "lab_result" not in st.session_state: st.session_state.lab_result = None
if "lab_metrics" not in st.session_state: st.session_state.lab_metrics = None

# --- LAYOUT PRINCIPAL ---
tab_chat, tab_lab, tab_manager = st.tabs(["üí¨ Chat Libre (Historique)", "üß™ Labo de Tests (One-Shot)", "‚öôÔ∏è Gestion des Mod√®les"])

# ==========================================
# ONGLET 1 : CHAT LIBRE (Stateful)
# ==========================================
with tab_chat:
    col_chat_params, col_chat_main = st.columns([1, 3])
    
    with col_chat_params:
        st.subheader("Param√®tres")
        # S√©lection Mod√®le
        installed = LLMProvider.list_models()
        model_map = {get_friendly_name_from_tag(m['model']): m['model'] for m in installed} if installed else {}
        selected_friendly = st.selectbox("Mod√®le actif", sorted(model_map.keys()), key="chat_model_select")
        selected_tag = model_map.get(selected_friendly)
        
        temp = st.slider("Temp√©rature", 0.0, 1.0, 0.7, key="chat_temp")
        
        st.info("Ce mode conserve l'historique de la conversation.")
        if st.button("üóëÔ∏è Nouvelle conversation", use_container_width=True):
            st.session_state.messages = []
            st.rerun()

    with col_chat_main:
        # Affichage Historique (Mise √† jour pour supporter le champ 'thought')
        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                # Si le message contient une pens√©e enregistr√©e, on l'affiche d'abord
                if "thought" in msg and msg["thought"]:
                    with st.expander("üí≠ Raisonnement (CoT)", expanded=False):
                        st.markdown(msg["thought"])
                st.markdown(msg["content"])

        # Input
        if prompt := st.chat_input("Discutez avec le mod√®le..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                msg_container = st.empty()
                
                # --- D√âBUT REMPLACEMENT ---
                if selected_tag:
                    # 1. D√©finition de la logique asynchrone qui RETOURNE le texte final
                    async def run_chat():
                        current_text = "" # Variable locale √† la fonction async
                        
                        # Appel du g√©n√©rateur async
                        stream = LLMProvider.chat_stream(
                            selected_tag, 
                            st.session_state.messages, 
                            temperature=temp
                        )
                        
                        async for item in stream:
                            if isinstance(item, str):
                                current_text += item
                                msg_container.markdown(current_text + "‚ñå")
                            elif isinstance(item, InferenceMetrics):
                                st.session_state.last_metrics = item
                        
                        return current_text

                    # 2. Ex√©cution et r√©cup√©ration du r√©sultat
                    import asyncio
                    full_text = asyncio.run(run_chat())
                    # --- FIN REMPLACEMENT ---

                    # 3. Nettoyage et Extraction de la Pens√©e
                    msg_container.empty()
                    thought, clean_text = extract_thought(full_text)
                    
                    # 4. Affichage structur√©
                    if thought:
                        with msg_container.container():
                            with st.expander("üí≠ Raisonnement (Chain of Thought)", expanded=True):
                                st.markdown(thought)
                            st.markdown(clean_text)
                    else:
                        msg_container.markdown(full_text)
                        clean_text = full_text # Fallback

                    # 5. Sauvegarde Historique
                    st.session_state.messages.append({
                        "role": "assistant", 
                        "content": clean_text,
                        "thought": thought
                    })
                    
                    # Force le rafra√Æchissement pour afficher les m√©triques
                    st.rerun()
                else:
                    st.error("S√©lectionnez un mod√®le.")

# ==========================================
# ONGLET 2 : LABO DE TESTS (Stateless)
# ==========================================
with tab_lab:
    col_lab_config, col_lab_run, col_lab_metrics = st.columns([1, 2, 1])

    # --- A. CONFIGURATION ---
    with col_lab_config:
        st.subheader("1. Sc√©nario")
        
        # Choix du mod√®le (Ind√©pendant du Chat)
        lab_model_friendly = st.selectbox("Mod√®le de Test", sorted(model_map.keys()), key="lab_model_select")
        lab_model_tag = model_map.get(lab_model_friendly)
        
        # Choix du Use Case
        selected_use_case = st.selectbox("Cas d'Usage", list(USE_CASES.keys()))
        
        # R√©cup√©ration des defaults
        default_sys = USE_CASES[selected_use_case]["system"]
        default_user = USE_CASES[selected_use_case]["user"]

        # Param√®tres d'ex√©cution
        lab_temp = st.slider("Temp√©rature", 0.0, 1.0, 0.2, key="lab_temp", help="Basse pour extraction/code, Haute pour cr√©ativit√©")

    # --- B. EX√âCUTION ---
    with col_lab_run:
        st.subheader("2. Entr√©es & Sorties")
        
        # Prompt Syst√®me √âditable
        with st.expander("üõ†Ô∏è Prompt Syst√®me (Configuration du comportement)", expanded=True):
            system_prompt = st.text_area("Instruction Syst√®me", value=default_sys, height=100)
            
        # Prompt Utilisateur √âditable (One Shot)
        user_prompt = st.text_area("Prompt Utilisateur (Entr√©e)", value=default_user, height=150)
        
        if st.button("üöÄ Lancer le Test (One-Shot)", use_container_width=True):
            if lab_model_tag:
                with st.spinner("Inf√©rence en cours..."):
                    # On construit un historique √©ph√©m√®re (Stateless)
                    messages = [{"role": "user", "content": user_prompt}]
                    
                    # --- D√âBUT MODIFICATION ASYNC (Labo de Tests) ---
                    placeholder = st.empty()
                    
                    async def run_lab_test():
                        current_text = ""
                        current_metrics = None
                        
                        # Appel Backend Asynchrone
                        stream = LLMProvider.chat_stream(
                            model_name=lab_model_tag,
                            messages=messages,
                            temperature=lab_temp,
                            system_prompt=system_prompt
                        )
                        
                        # Consommation du stream
                        async for item in stream:
                            if isinstance(item, str):
                                current_text += item
                                # Mise √† jour UI en temps r√©el
                                placeholder.markdown(current_text + "‚ñå")
                            elif isinstance(item, InferenceMetrics):
                                current_metrics = item
                        
                        return current_text, current_metrics

                    # Ex√©cution via la boucle d'√©v√©nements
                    import asyncio
                    full_resp, metrics = asyncio.run(run_lab_test())
                    # --- FIN MODIFICATION ASYNC ---
                    
                    # 2. Nettoyage et Extraction
                    placeholder.empty()
                    thought, clean_text = extract_thought(full_resp)
                    
                    # 3. Affichage structur√© final
                    if thought:
                        with placeholder.container():
                            with st.expander("üí≠ Raisonnement", expanded=True):
                                st.markdown(thought)
                            st.markdown(clean_text)
                    else:
                        placeholder.markdown(full_resp)
                    
                    # Sauvegarde dans le Session State pour les m√©triques √† droite
                    st.session_state.lab_result = full_resp 
                    st.session_state.lab_metrics = metrics
            else:
                st.warning("Aucun mod√®le s√©lectionn√©.")

    # --- C. M√âTRIQUES ---
    with col_lab_metrics:
        st.subheader("3. Audit")
        
        m = st.session_state.lab_metrics
        if m:
            # R√©cup√©ration taille mod√®le
            info = get_model_info(lab_model_friendly)
            size_gb = info['size_gb'] if info else "?"

            # Vitesse
            st.markdown("#### ‚ö° Performance")
            st.metric("D√©bit (t/s)", f"{m.tokens_per_second}", delta="Fluide" if m.tokens_per_second > 20 else "Lent")
            st.metric("Latence Totale", f"{m.total_duration_s} s")
            
            # Technique
            st.markdown("#### üíª Technique")
            st.text(f"Load Time: {m.load_duration_s}s")
            st.text(f"In Tokens: {m.input_tokens}")
            st.text(f"Out Tokens: {m.output_tokens}")
            st.metric("RAM Mod√®le", size_gb)
            
            # Green IT
            st.markdown("#### üå± Impact")
            st.caption("Estimation √©nerg√©tique")
            st.progress(0.1, text="Calcul CodeCarbon...") # Placeholder
        else:
            st.info("Lancez un test pour voir les m√©triques.")
            st.markdown("""
            > **Note :** Les tests 'One-Shot' ne gardent pas de m√©moire. Chaque clic sur 'Lancer' repart d'une feuille blanche.
            """)

# ==========================================
# ONGLET 3 : GESTIONNAIRE DE MOD√àLES (MANAGER)
# ==========================================
with tab_manager:
    st.markdown("### üì¶ Mod√®les Install√©s & Documentation")
    
    col_refresh, col_filter = st.columns([1, 3])
    with col_refresh:
        if st.button("üîÑ Rafra√Æchir la liste"):
            st.rerun()
            
    # Filtres
    with col_filter:
        all_langs = get_all_languages()
        selected_langs = st.multiselect("üåç Filtrer par langue support√©e (ET logique)", all_langs)

    installed_models = LLMProvider.list_models()
    
    if installed_models:
        table_data = []
        for m in installed_models:
            tag = m['model']
            friendly_name = get_friendly_name_from_tag(tag)
            
            info = get_model_info(friendly_name)
            
            # --- R√âCUP√âRATION DES STATS DE BENCHMARK (M√™me si 'info' est None) ---
            benchmark_stats = info.get("benchmark_stats", {}) if info else {}
            
            row = {
                "Nom": friendly_name,
                "√âditeur": "Inconnu",
                "Taille": f"{round(m.get('size', 0) / (1024**3), 2)} GB",
                
                # --- NOUVELLES COLONNES DE BENCHMARK ---
                "RAM (GB)": benchmark_stats.get('ram_usage_gb', None),
                "Vitesse (s)": benchmark_stats.get('speed_s', None),
                "CO2 (kg)": benchmark_stats.get('co2_emissions_kg', None),
                "Max Contexte (tk)": benchmark_stats.get('tested_ctx', info.get('ctx', 'N/A') if info else 'N/A'),
                
                "Total Params": "N/A", # Remplacement de 'Params Tot.'
                "Actifs": "N/A",       # Remplacement de 'Params Act.'
                "Contexte": "N/A",
                "Langues": [],
                "Description": "Mod√®le t√©l√©charg√© manuellement.",
                "Documentation": None
            }
            
            if info:
                # Ici on ne garde que la logique de filtrage et de remplissage de base
                if selected_langs:
                    model_langs = set(info.get("langs", []))
                    if not set(selected_langs).issubset(model_langs):
                        continue

                # Remplissage des champs de documentation
                row["√âditeur"] = info.get("editor", "N/A")
                row["Total Params"] = info.get("params_tot", "N/A") # Cl√© mise √† jour
                row["Actifs"] = info.get("params_act", "N/A")       # Cl√© mise √† jour
                row["Langues"] = info.get("langs", [])
                row["Description"] = info.get("desc", "")
                row["Documentation"] = info.get("link", None)
            
            elif selected_langs:
                continue

            table_data.append(row)

        if table_data:
            df = pd.DataFrame(table_data)
            # D√©finition de l'ordre des colonnes (les m√©triques d'abord)
            column_order = [
                "Nom", "√âditeur", "Taille", 
                "RAM (GB)", "Max Contexte (tk)", 
                "Vitesse (s)", "CO2 (kg)", 
                "Total Params", "Actifs", 
                "Langues", "Description", "Documentation"
            ]
            
            st.dataframe(
                df,
                use_container_width=True,
                column_order=column_order,
                column_config={
                    # --- NOUVELLES CONFIGURATIONS DE BENCHMARK ---
                    "RAM (GB)": st.column_config.NumberColumn(
                        "RAM Mod√®le (GB)",
                        help="RAM utilis√©e au Max Valid Context (GB).",
                        format="%.2f", 
                        width="small" # R√©duire la largeur pour gagner de la place
                    ),
                    "Max Contexte (tk)": st.column_config.NumberColumn(
                        "Max Contexte (tk)",
                        help="Taille maximale du contexte valid√© (tokens).",
                        format="%d",
                        width="small"
                    ),
                    "Vitesse (s)": st.column_config.NumberColumn(
                        "Vitesse (s)",
                        help="Dur√©e totale du test au Max Valid Context (s). Plus bas = Mieux.",
                        format="%.2f",
                        width="small"
                    ),
                    "CO2 (kg)": st.column_config.NumberColumn(
                        "CO2 (kg)",
                        help="√âmissions cumul√©es pour l'audit complet du contexte (kg CO2).",
                        format="%.3g", 
                        width="small"
                    ),
                    
                    # --- CONFIGURATIONS DESCRIPTIVES ---
                    "Total Params": st.column_config.TextColumn("Total Params", width="small"),
                    "Actifs": st.column_config.TextColumn("Actifs", width="small"),
                    "Langues": st.column_config.ListColumn("Langues", width="medium"),
                    "Description": st.column_config.TextColumn("Description", width="large"),
                    "Documentation": st.column_config.LinkColumn("Lien Doc", display_text="Voir Fiche"),
                    "Taille": st.column_config.TextColumn("Disk (GB)", help="Espace disque occup√©", width="small")
                },
                hide_index=True
            )
        else:
            st.warning("Aucun mod√®le ne correspond aux filtres.")
    else:
        st.info("Aucun mod√®le local trouv√©.")
        
    st.markdown("---")
    st.markdown("### ‚¨áÔ∏è T√©l√©charger un nouveau mod√®le")
    
    col_select, col_info = st.columns([1, 1])
    with col_select:
        suggestions = sorted(get_all_friendly_names(local_only=True))
        options = ["‚ú® S√©lectionner une suggestion..."] + suggestions + ["üõ†Ô∏è Autre (Saisie Manuelle)"]
        choice = st.selectbox("Catalogue Wavestone", options)
        
        target_model_tag = ""
        if choice == "üõ†Ô∏è Autre (Saisie Manuelle)":
            target_model_tag = st.text_input("Tag Ollama", "")
            st.caption("[Ollama Library](https://ollama.com/library)")
        elif choice != "‚ú® S√©lectionner une suggestion...":
            info = get_model_info(choice)
            if info: target_model_tag = info["ollama_tag"]
            
    with col_info:
        if choice not in ["‚ú® S√©lectionner une suggestion...", "üõ†Ô∏è Autre (Saisie Manuelle)"]:
            info = get_model_info(choice)
            if info:
                st.info(f"**{info['desc']}**")
                st.markdown(f"**Contexte:** `{info['ctx']}` | **Params:** `{info['params_tot']}`")
                
    st.write("")
    if st.button("‚¨áÔ∏è Lancer le t√©l√©chargement"):
        if target_model_tag:
            status = st.status(f"T√©l√©chargement de {target_model_tag}...", expanded=True)
            pbar = status.progress(0, text="Connexion...")
            try:
                for progress in LLMProvider.pull_model(target_model_tag):
                    if progress.get('total'):
                        p = progress['completed'] / progress['total']
                        pbar.progress(p, text=f"{progress['status']} - {int(p*100)}%")
                    else:
                        pbar.progress(0.5, text=progress['status'])
                pbar.progress(1.0, text="Termin√© !")
                status.update(label="‚úÖ Succ√®s !", state="complete", expanded=False)
                time.sleep(1)
                st.rerun()
            except Exception as e:
                status.update(label="‚ùå Erreur", state="error")
                st.error(str(e))