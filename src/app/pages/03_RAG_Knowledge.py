"""
RAG Knowledge Tab - Sprint 1 (Onboarding & Ingestion)
Changements :
- Upload via st.dialog (Modal)
- Sidebar nettoyÃ©e (Options avancÃ©es repliÃ©es)
- Gestion de l'Ã©tat vide ("Empty State")
"""

import time

import nest_asyncio
import streamlit as st

# UI COMPONENTS
from src.app.tabs.rag.chat import render_rag_chat_tab
from src.app.tabs.rag.eval import render_rag_eval_tab
from src.core.config import DATA_DIR
from src.core.eval_engine import EvalEngine
from src.core.llm_provider import LLMProvider
from src.core.models_db import get_friendly_name_from_tag
from src.core.rag.strategies.hyde import HyDERetrievalStrategy
from src.core.rag.strategies.naive import NaiveRetrievalStrategy
from src.core.rag.strategies.self_rag import SelfRAGStrategy
from src.core.rag_engine import RAGEngine

# PATCH ASYNCIO
nest_asyncio.apply()

st.set_page_config(page_title="RAG Knowledge Base", page_icon="ğŸ§ ", layout="wide")

# --- CSS CUSTOM (Empty State & Metrics) ---
st.markdown(
    """
<style>
    div[data-testid="stMetricValue"] { font-size: 1.2rem; }
    .big-icon { font-size: 4rem; text-align: center; display: block; margin-bottom: 1rem; }
    .empty-state-box {
        border: 2px dashed #4b4b4b;
        border-radius: 10px;
        padding: 3rem;
        text-align: center;
        margin-top: 2rem;
        background-color: #262730;
    }
</style>
""",
    unsafe_allow_html=True,
)


# --- 0. HELPERS ---
def get_local_models(subfolder: str):
    path = DATA_DIR / "models" / subfolder
    if not path.exists():
        return []
    return [d.name for d in path.iterdir() if d.is_dir()]


# --- 1. INITIALISATION SERVICES ---
if "rag_engine" not in st.session_state:
    with st.spinner("ğŸš€ DÃ©marrage du moteur RAG..."):
        avail_emb = get_local_models("embeddings")
        default_emb = (
            "bge-m3"
            if "bge-m3" in avail_emb
            else (avail_emb[0] if avail_emb else "all-MiniLM-L6-v2")
        )
        avail_rerank = get_local_models("rerankers")
        default_rerank = avail_rerank[0] if avail_rerank else None

        st.session_state.rag_engine = RAGEngine(
            embedding_model_name=default_emb, reranker_model_name=default_rerank
        )

if "eval_engine" not in st.session_state:
    try:
        st.session_state.eval_engine = EvalEngine()
    except Exception:
        # Optionnel : loguer l'erreur pour le dÃ©bogage
        # print(f"Erreur lors de la lecture du systÃ¨me: {e}")
        st.session_state.eval_engine = None

if "rag_messages" not in st.session_state:
    st.session_state.rag_messages = []


# --- 2. MODAL D'INGESTION (NOUVEAU) ---
@st.dialog("ğŸ“‚ Gestion de la Base de Connaissance")
def open_knowledge_manager():
    st.caption("Ajoutez des documents PDF, TXT ou MD pour nourrir le cerveau de l'IA.")

    # Zone d'upload large
    uploaded_files = st.file_uploader(
        "SÃ©lectionner des fichiers", type=["pdf", "txt", "md", "docx"], accept_multiple_files=True
    )

    if uploaded_files:
        st.info(f"ğŸ“„ {len(uploaded_files)} fichier(s) prÃªt(s) Ã  Ãªtre indexÃ©(s).")

        if st.button("ğŸš€ Indexer maintenant", type="primary", use_container_width=True):
            # Simulation d'ingestion (Remplacer par votre appel rÃ©el rag_engine.add_documents)
            progress_bar = st.progress(0)
            status_text = st.empty()

            try:
                # Exemple de boucle d'ingestion
                for i, file in enumerate(uploaded_files):
                    status_text.text(f"Traitement de {file.name}...")
                    # --- CODE D'INGESTION RÃ‰EL ICI ---
                    # rag_engine.ingest(file)
                    # ---------------------------------
                    time.sleep(0.5)  # Fake work pour la dÃ©mo UX
                    progress_bar.progress((i + 1) / len(uploaded_files))

                st.success("âœ… Indexation terminÃ©e avec succÃ¨s !")
                time.sleep(1)
                st.rerun()
            except Exception as e:
                st.error(f"Erreur lors de l'indexation : {e}")

    st.divider()
    st.caption("Statistiques actuelles :")
    stats = st.session_state.rag_engine.get_stats()
    st.markdown(f"**{stats['count']} documents** dans la collection active.")

    if st.button("ğŸ—‘ï¸ Tout supprimer (Reset)", type="secondary"):
        st.session_state.rag_engine.clear_database()
        st.rerun()


# --- 3. SIDEBAR (NETTOYÃ‰E) ---
with st.sidebar:
    st.header("ğŸ›ï¸ Configuration RAG")

    # A. Mode Cloud/Local
    if "cloud_enabled" not in st.session_state:
        st.session_state.cloud_enabled = True
    st.session_state.cloud_enabled = st.toggle(
        "Activer Cloud (Mistral)",
        value=st.session_state.cloud_enabled,
        help="Si dÃ©sactivÃ©, seuls les modÃ¨les locaux (Ollama) seront accessibles.",
    )
    if not st.session_state.cloud_enabled:
        st.caption("ğŸ”’ Local Only (Ollama)")

    st.divider()

    # B. Action Principale (Gros Bouton)
    st.markdown("#### ğŸ“š Base de Connaissance")
    if st.button("ğŸ“‚ GÃ©rer les Documents", type="primary", use_container_width=True, icon="ğŸ“‚"):
        open_knowledge_manager()

    # Stats Rapides
    stats = st.session_state.rag_engine.get_stats()
    st.caption(f"ğŸ“Š **{stats['count']}** chunks indexÃ©s")

    st.divider()

    # C. ParamÃ¨tres AvancÃ©s (RepliÃ©s)
    with st.expander("âš™ï¸ RÃ©glages AvancÃ©s (Experts)", expanded=False):
        # 1. Embedding
        st.caption("Cerveau Documentaire (Embedding)")
        avail_emb = get_local_models("embeddings") or ["sentence-transformers/all-MiniLM-L6-v2"]
        curr_emb = st.session_state.rag_engine.current_embedding_name
        sel_emb = st.selectbox(
            "ModÃ¨le",
            avail_emb,
            index=avail_emb.index(curr_emb) if curr_emb in avail_emb else 0,
            label_visibility="collapsed",
        )

        if sel_emb != curr_emb:
            st.session_state.rag_engine.set_models(embedding_name=sel_emb)
            st.rerun()

        st.markdown("---")

        # 2. StratÃ©gie
        st.caption("StratÃ©gie de Recherche")
        strat_mode = st.radio("Mode", ["Naive RAG", "HyDE", "Self-RAG"], index=0)
        k_retrieval = st.slider("Top-K Chunks", 1, 10, 4)

        # 3. Reranker
        st.caption("Reranker (Affinement)")
        avail_rerank = ["Aucun"] + get_local_models("rerankers")
        curr_rerank = st.session_state.rag_engine.current_reranker_name
        sel_rerank = st.selectbox(
            "ModÃ¨le",
            avail_rerank,
            index=avail_rerank.index(curr_rerank) if curr_rerank in avail_rerank else 0,
        )

        # Apply logic
        if strat_mode == "Naive RAG":
            st.session_state.rag_engine.set_strategy(NaiveRetrievalStrategy())
        elif strat_mode == "HyDE":
            st.session_state.rag_engine.set_strategy(HyDERetrievalStrategy())
        elif strat_mode == "Self-RAG":
            st.session_state.rag_engine.set_strategy(SelfRAGStrategy())

        # Reranker change logic would go here if needed per existing code

# --- 4. MAIN PAGE LOGIC ---

st.title("ğŸ§  Assistant Documentaire")

# VÃ©rification de l'Ã©tat vide
doc_count = st.session_state.rag_engine.get_stats()["count"]

if doc_count == 0:
    # --- EMPTY STATE UI ---
    st.markdown(
        """
        <div class="empty-state-box">
            <div class="big-icon">ğŸ“­</div>
            <h2>Votre base de connaissances est vide</h2>
            <p style="color: #cccccc;">Pour commencer Ã  discuter avec vos documents, vous devez d'abord les importer.</p>
        </div>
    """,
        unsafe_allow_html=True,
    )

    col_center = st.columns([1, 2, 1])
    with col_center[1]:
        if st.button("ğŸš€ Commencer l'ingestion", type="primary", use_container_width=True):
            open_knowledge_manager()

    st.markdown("### ğŸ’¡ Pourquoi utiliser le RAG ?")
    c1, c2, c3 = st.columns(3)
    with c1:
        st.info(
            "**ğŸ”’ ConfidentialitÃ©**\n\nVos documents restent en local, aucune fuite de donnÃ©es."
        )
    with c2:
        st.info("**âš¡ PrÃ©cision**\n\nLe modÃ¨le rÃ©pond uniquement basÃ© sur VOS sources vÃ©rifiÃ©es.")
    with c3:
        st.info(
            "**ğŸŒ± GreenOps**\n\nUtilisez des petits modÃ¨les prÃ©cis plutÃ´t que des monstres Ã©nergivores."
        )

else:
    # --- NORMAL UI (TABS) ---
    installed_models_list = LLMProvider.list_models(cloud_enabled=st.session_state.cloud_enabled)

    def format_model_label(m):
        icon = "â˜ï¸" if m.get("type") in ["cloud", "api"] else "ğŸ’»"
        return f"{icon} {get_friendly_name_from_tag(m['model'])}"

    display_to_tag = {format_model_label(m): m["model"] for m in installed_models_list}
    tag_to_friendly = {
        m["model"]: get_friendly_name_from_tag(m["model"]) for m in installed_models_list
    }
    sorted_display_names = sorted(display_to_tag.keys())

    tab_chat, tab_eval = st.tabs(["ğŸ’¬ Discussion", "âš–ï¸ Benchmark & QualitÃ©"])

    with tab_chat:
        render_rag_chat_tab(
            st.session_state.rag_engine,
            display_to_tag,
            tag_to_friendly,
            sorted_display_names,
            k_retrieval,
        )

    with tab_eval:
        render_rag_eval_tab(
            st.session_state.rag_engine,
            st.session_state.eval_engine,
            display_to_tag,
            tag_to_friendly,
            sorted_display_names,
        )
