import asyncio

import streamlit as st

from src.core.inference_service import InferenceCallbacks, InferenceService
from src.core.models_db import get_model_info

# DonnÃ©es statiques (Cas d'usage)
USE_CASES = {
    "ðŸ“Š Classification Verbatims (JSON)": {
        "system": """Tu es un expert en analyse de feedback post-formation.
    Ton objectif est d'analyser une liste de commentaires bruts au format JSON.
    Pour chaque commentaire, tu dois produire un objet JSON contenant deux champs :
    1. "sentiment" : Uniquement 'Positif', 'Neutre' ou 'NÃ©gatif'.
    2. "categorie" : La thÃ©matique principale parmi ['Contenu', 'Animateur', 'Logistique', 'ApplicabilitÃ©', 'Technique'].

    RÃ©ponds UNIQUEMENT avec le JSON final minifiÃ©, sans markdown, sans introduction.""",
        "user": """{
        "1": "La formation Ã©tait top, j'ai tout compris sur les prompts.",
        "2": "Le formateur parlait trop vite, difficile de suivre.",
        "3": "Copilot est impressionnant mais je ne vois pas l'usage dans mon mÃ©tier.",
        "4": "La salle Ã©tait trop chaude, impossible de se concentrer.",
        "5": "TrÃ¨s utile, je gagne dÃ©jÃ  du temps sur mes mails.",
        "6": "L'outil a plantÃ© deux fois pendant la dÃ©mo...",
        "7": "C'Ã©tait correct, sans plus.",
        "8": "Les exemples concrets sur Excel Ã©taient pertinents.",
        "9": "Je n'ai pas reÃ§u le support de prÃ©sentation promis.",
        "10": "GÃ©nial, mais Ã§a fait peur pour l'avenir de nos jobs !"
    }""",
    },
    "ðŸ‡¬ðŸ‡§ Traduction Technique": {
        "system": 'Tu es un expert en traduction technique. Traduis le texte suivant en Anglais, Espagnol et Allemand. Sois prÃ©cis sur la terminologie informatique. RÃ©ponds au format JSON : {"en": "...", "es": "...", "de": "..."}.',
        "user": "L'architecture 'Local First' permet de rÃ©duire la latence rÃ©seau et d'amÃ©liorer la confidentialitÃ© des donnÃ©es en traitant les infÃ©rences directement sur le CPU de l'utilisateur, sans appel API vers le cloud.",
    },
    "ðŸ“„ Extraction StructurÃ©e (JSON)": {
        "system": "Tu es un extracteur de donnÃ©es strict. Extrais les entitÃ©s du texte (Date, Montant, Vendeur, Articles). RÃ©ponds UNIQUEMENT avec un JSON valide. Pas de texte avant ni aprÃ¨s.",
        "user": "FACTURE NÂ° 2024-001\nDate : 12 dÃ©cembre 2024\nVendeur : Wavestone Tech\n\nArticles :\n- 1x Audit Green IT (500â‚¬)\n- 3x Licences Copilot (90â‚¬)\n\nTotal TTC : 590â‚¬",
    },
    "ðŸ’» Assistant Coding (Python)": {
        "system": "Tu es un Tech Lead Python expÃ©rimentÃ©. GÃ©nÃ¨re du code propre, typÃ© (Type Hints) et documentÃ© (Docstrings). Inclus une gestion d'erreur robuste.",
        "user": "Ã‰cris une fonction Python asynchrone qui interroge une API REST avec la librairie 'httpx', gÃ¨re les retries en cas d'erreur 500, et retourne le rÃ©sultat en dictionnaire.",
    },
    "ðŸ§® Raisonnement (Chain of Thought)": {
        "system": "Tu es un expert en logique. Pour rÃ©pondre, tu dois IMPÃ‰RATIVEMENT utiliser la mÃ©thode 'Chain of Thought' : explique ton raisonnement Ã©tape par Ã©tape avant de donner la rÃ©ponse finale.",
        "user": "J'ai 3 pommes. Hier j'en ai mangÃ© une. Aujourd'hui j'en achÃ¨te deux autres, mais j'en fais tomber une dans la boue que je jette. Combien de pommes puis-je manger maintenant ?",
    },
    "ðŸ“ RÃ©sumÃ© ExÃ©cutif": {
        "system": "Tu es un assistant de direction. Fais un rÃ©sumÃ© concis (bullet points) du texte fourni, en te concentrant sur les dÃ©cisions clÃ©s et les actions Ã  entreprendre.",
        "user": "Compte rendu de rÃ©union - Projet Alpha.\nLa rÃ©union a dÃ©butÃ© Ã  10h. L'Ã©quipe a convenu que le budget initial Ã©tait insuffisant. Marc doit revoir le fichier Excel d'ici mardi. Sophie a soulevÃ© un risque de sÃ©curitÃ© sur l'API, il faut auditer le module d'auth. La deadline du projet est repoussÃ©e de 2 semaines pour permettre ces ajustements. Le client a validÃ© le nouveau design.",
    },
}


def render_lab_tab(sorted_display_names: list, display_to_tag: dict, tag_to_friendly: dict):
    col_lab_config, col_lab_run, col_lab_metrics = st.columns([1, 2, 1])

    with col_lab_config:
        st.subheader("1. ScÃ©nario")
        lab_model_display = st.selectbox(
            "ModÃ¨le de Test", sorted_display_names, key="lab_model_select"
        )
        lab_model_tag = display_to_tag.get(lab_model_display)
        lab_model_friendly = tag_to_friendly.get(lab_model_tag, "Inconnu")

        selected_use_case = st.selectbox("Cas d'Usage", list(USE_CASES.keys()))
        default_sys = USE_CASES[selected_use_case]["system"]
        default_user = USE_CASES[selected_use_case]["user"]

        lab_temp = st.slider(
            "TempÃ©rature",
            0.0,
            1.0,
            0.2,
            key="lab_temp",
            help="Basse pour extraction/code, Haute pour crÃ©ativitÃ©",
        )

    with col_lab_run:
        st.subheader("2. EntrÃ©es & Sorties")
        with st.expander("ðŸ› ï¸ Prompt SystÃ¨me", expanded=True):
            system_prompt = st.text_area("Instruction SystÃ¨me", value=default_sys, height=100)
        user_prompt = st.text_area("Prompt Utilisateur", value=default_user, height=150)

        if st.button("ðŸš€ Lancer le Test (One-Shot)", use_container_width=True):
            if lab_model_tag:
                placeholder = st.empty()
                state = {"current_text": ""}

                async def on_token(token: str):
                    state["current_text"] += token
                    placeholder.markdown(state["current_text"] + "â–Œ")

                callbacks = InferenceCallbacks(on_token=on_token)
                messages = [{"role": "user", "content": user_prompt}]

                with st.spinner("InfÃ©rence en cours..."):
                    result = asyncio.run(
                        InferenceService.run_inference(
                            model_tag=lab_model_tag,
                            messages=messages,
                            temperature=lab_temp,
                            system_prompt=system_prompt,
                            callbacks=callbacks,
                        )
                    )

                placeholder.empty()
                if result.thought:
                    with placeholder.container(), st.expander("ðŸ’­ Raisonnement", expanded=True):
                        st.markdown(result.thought)
                    st.markdown(result.clean_text)
                else:
                    placeholder.markdown(result.clean_text)

                st.session_state.lab_result = result.raw_text
                st.session_state.lab_metrics = result.metrics
            else:
                st.warning("SÃ©lectionnez un modÃ¨le.")

    with col_lab_metrics:
        st.subheader("3. Audit")
        m = st.session_state.lab_metrics
        if m:
            info = get_model_info(lab_model_friendly)
            size_gb = info["size_gb"] if info else "?"

            st.markdown("#### âš¡ Performance")
            st.metric(
                "DÃ©bit (t/s)",
                f"{m.tokens_per_second}",
                delta="Fluide" if m.tokens_per_second > 20 else "Lent",
            )
            st.metric("Latence Totale", f"{m.total_duration_s} s")

            st.markdown("#### ðŸ’» Technique")
            st.text(f"Load Time: {m.load_duration_s}s")
            st.text(f"In Tokens: {m.input_tokens}")
            st.text(f"Out Tokens: {m.output_tokens}")
            st.metric("RAM ModÃ¨le", size_gb)

            st.markdown("#### ðŸŒ± Impact")
            st.progress(0.1, text="Calcul CodeCarbon...")
        else:
            st.info("Lancez un test pour voir les mÃ©triques.")
