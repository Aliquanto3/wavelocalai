"""
Inference Lab Tab - Sprint 2 (Ergonomie & ClartÃ©)
Refonte UX : Layout 2 colonnes (Input vs Output), MÃ©triques en tÃªte de rÃ©sultat.
"""

import asyncio

import streamlit as st

from src.core.green_monitor import CarbonCalculator
from src.core.inference_service import InferenceCallbacks, InferenceService
from src.core.models_db import get_model_info


# --- HELPER PARSING ---
def _extract_params_billions(val: str | int | float) -> float:
    if isinstance(val, (int, float)):
        return float(val)
    if not val or not isinstance(val, str):
        return 0.0
    s = val.upper().strip().replace(" ", "")
    try:
        if "X" in s and "B" in s:
            parts = s.replace("B", "").split("X")
            return float(parts[0]) * float(parts[1])
        if s.endswith("B"):
            return float(s[:-1])
        if s.endswith("M"):
            return float(s[:-1]) / 1000.0
        if s.isdigit():
            return float(s)
    except Exception:
        pass
    return 0.0


# --- DONNÃ‰ES SCÃ‰NARIOS ---
USE_CASES = {
    "ğŸ“Š Classification (JSON)": {
        "system": """Tu es un expert en analyse de sentiment. RÃ©ponds UNIQUEMENT avec un JSON : {"sentiment": "Positif"|"Neutre"|"NÃ©gatif", "categorie": "..."}.""",
        "user": """Analyse ce feedback : "La formation Ã©tait top, mais la salle trop chaude." """,
    },
    "ğŸ‡¬ğŸ‡§ Traduction Technique": {
        "system": 'Traduis en Anglais, Espagnol, Allemand. Format JSON : {"en": "...", "es": "...", "de": "..."}.',
        "user": "L'infÃ©rence locale garantit la confidentialitÃ© des donnÃ©es.",
    },
    "ğŸ“„ Extraction (JSON)": {
        "system": "Extrais les entitÃ©s (Date, Montant, Vendeur). RÃ©ponds UNIQUEMENT en JSON.",
        "user": "Facture du 12/12/2024 de Wavestone pour 500â‚¬.",
    },
    "ğŸ’» Assistant Code (Python)": {
        "system": "Tu es un expert Python. GÃ©nÃ¨re du code typÃ© et documentÃ©.",
        "user": "Fonction asynchrone pour appeler une API REST avec retry.",
    },
    "ğŸ§® Raisonnement (CoT)": {
        "system": "Utilise la mÃ©thode Chain of Thought : pense Ã©tape par Ã©tape avant de rÃ©pondre.",
        "user": "J'ai 3 pommes. J'en mange une. J'en achÃ¨te deux. J'en jette une. Combien m'en reste-t-il ?",
    },
    "ğŸ“ RÃ©sumÃ©": {
        "system": "Fais un rÃ©sumÃ© exÃ©cutif en bullet points.",
        "user": "Compte rendu de rÃ©union : Le projet est en retard Ã  cause de la validation API. On dÃ©cale la livraison de 2 semaines.",
    },
}


def render_lab_tab(sorted_display_names: list, display_to_tag: dict, tag_to_friendly: dict):

    # --- LAYOUT ASYMÃ‰TRIQUE (40% Input / 60% Output) ---
    col_input, col_output = st.columns([2, 3])

    # === COLONNE GAUCHE : CONFIGURATION ===
    with col_input:
        st.subheader("1. Configuration")

        # SÃ©lection ModÃ¨le & Cas
        lab_model_display = st.selectbox("ModÃ¨le", sorted_display_names, key="lab_model_select")
        lab_model_tag = display_to_tag.get(lab_model_display)
        lab_model_friendly = tag_to_friendly.get(lab_model_tag, "Inconnu")

        selected_use_case = st.selectbox("ScÃ©nario PrÃ©dÃ©fini", list(USE_CASES.keys()))
        default_sys = USE_CASES[selected_use_case]["system"]
        default_user = USE_CASES[selected_use_case]["user"]

        # ParamÃ¨tres avancÃ©s cachÃ©s
        with st.expander("âš™ï¸ ParamÃ¨tres (System & Temp)", expanded=False):
            system_prompt = st.text_area("System Prompt", value=default_sys, height=100)
            lab_temp = st.slider("TempÃ©rature", 0.0, 1.0, 0.2)

        # Zone de Prompt User
        st.markdown("**EntrÃ©e Utilisateur**")
        user_prompt = st.text_area(
            "Votre prompt", value=default_user, height=200, label_visibility="collapsed"
        )

        # Bouton Action
        if st.button("ğŸš€ Lancer le Test", type="primary", use_container_width=True):
            if lab_model_tag:
                st.session_state.lab_trigger = True
            else:
                st.warning("SÃ©lectionnez un modÃ¨le.")

    # === COLONNE DROITE : RÃ‰SULTAT ===
    with col_output:
        st.subheader("2. RÃ©sultat & Analyse")

        # Container de rÃ©sultat
        res_container = st.container(border=True)

        if st.session_state.get("lab_trigger"):
            # Reset trigger
            st.session_state.lab_trigger = False

            with res_container:
                placeholder = st.empty()
                state = {"text": ""}

                async def on_token(token: str):
                    state["text"] += token
                    placeholder.markdown(state["text"] + "â–Œ")

                callbacks = InferenceCallbacks(on_token=on_token)

                # RUN
                with st.spinner("GÃ©nÃ©ration..."):
                    result = asyncio.run(
                        InferenceService.run_inference(
                            model_tag=lab_model_tag,
                            messages=[{"role": "user", "content": user_prompt}],
                            temperature=lab_temp,
                            system_prompt=system_prompt,
                            callbacks=callbacks,
                        )
                    )

                # Affichage Final (Clean)
                placeholder.empty()
                if result.thought:
                    with st.expander("ğŸ’­ Raisonnement du modÃ¨le", expanded=True):
                        st.markdown(result.thought)

                st.markdown("### RÃ©ponse")
                st.markdown(result.clean_text)

                # Sauvegarde Ã©tat pour affichage persistant
                st.session_state.lab_last_result = result
                st.session_state.lab_last_model = lab_model_friendly

        # Affichage Persistant (si un rÃ©sultat existe dÃ©jÃ )
        elif "lab_last_result" in st.session_state:
            res = st.session_state.lab_last_result
            with res_container:
                if res.thought:
                    with st.expander("ğŸ’­ Raisonnement du modÃ¨le", expanded=False):
                        st.markdown(res.thought)
                st.markdown("### RÃ©ponse")
                st.markdown(res.clean_text)

        # === ZONE MÃ‰TRIQUES (Sous le rÃ©sultat) ===
        if "lab_last_result" in st.session_state:
            res = st.session_state.lab_last_result
            model_name = st.session_state.lab_last_model
            m = res.metrics
            info = get_model_info(model_name) or {}

            st.divider()

            # Calcul CO2
            is_api = info.get("type") == "api"
            carbon_mg = 0.0
            if is_api and m.output_tokens > 0:
                p = _extract_params_billions(info.get("params_act") or info.get("params_tot", "0"))
                carbon_mg = CarbonCalculator.compute_mistral_impact_g(p, m.output_tokens) * 1000
            else:
                carbon_mg = CarbonCalculator.compute_local_theoretical_g(m.output_tokens) * 1000

            # Affichage en Grid
            c1, c2, c3, c4 = st.columns(4)
            c1.metric("âš¡ Vitesse", f"{m.tokens_per_second:.1f} t/s")
            c2.metric("ğŸŒ± Impact", f"{carbon_mg:.2f} mg")
            c3.metric("â±ï¸ Latence", f"{m.total_duration_s:.2f} s")
            c4.metric("ğŸ“ Tokens", f"{m.output_tokens}")

        else:
            with res_container:
                st.info("ğŸ‘ˆ Configurez et lancez le test pour voir le rÃ©sultat.")
