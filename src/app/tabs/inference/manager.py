import time

import pandas as pd
import streamlit as st

from src.core.llm_provider import LLMProvider
from src.core.models_db import (
    get_all_friendly_names,
    get_all_languages,
    get_model_card,
    get_model_info,
)


def _parse_params_to_float(val: str | int | float) -> float:
    """Convertit '7B', '350M' en float (Milliards) pour le tri."""
    if isinstance(val, (int, float)):
        return float(val)
    if not val or not isinstance(val, str):
        return 0.0
    s = val.upper().strip().replace(" ", "")
    try:
        if "X" in s and "B" in s:  # Ex: 8x7B
            parts = s.replace("B", "").split("X")
            return float(parts[0]) * float(parts[1])
        if s.endswith("B"):
            return float(s[:-1])
        if s.endswith("M"):
            return float(s[:-1]) / 1000.0
        if s.isdigit():
            return float(s)
    except Exception:
        pass
    return 0.0


def _parse_size_to_float(val: str) -> float:
    """Convertit '1.5 GB' en 1.5 (float)."""
    if not val or not isinstance(val, str):
        return 0.0
    try:
        return float(val.lower().replace("gb", "").replace("mb", "").strip())
    except Exception:
        return 0.0


def render_manager_tab(installed_models_list: list):
    st.markdown("### üì¶ Biblioth√®que de Mod√®les")

    # 1. Filtres
    col_filter_lang, col_filter_cap, col_refresh, col_kpi = st.columns([2, 2, 1, 1])

    with col_filter_lang:
        all_langs = get_all_languages()
        selected_langs = st.multiselect(
            "üåç Langues requises (ET)",
            all_langs,
            help="Affiche uniquement les mod√®les valid√©s pour TOUTES ces langues.",
        )

    with col_filter_cap:
        cap_options = {
            "üõ†Ô∏è Tools": "tools_validated",
            "{} JSON": "json_validated",
            "üß† Raisonnement": "reasoning_high",
            "‚ö° Rapide (<800ms)": "fast_ux",
        }
        selected_caps = st.multiselect(
            "‚ú® Capacit√©s requises",
            options=list(cap_options.keys()),
            help="Filtre sur les capacit√©s techniques valid√©es.",
        )

    with col_refresh:
        st.write("")
        if st.button("üîÑ Rafra√Æchir", use_container_width=True):
            st.rerun()

    with col_kpi:
        st.write("")
        st.metric("Total", len(installed_models_list), label_visibility="collapsed")

    # 2. Pr√©paration des donn√©es
    if installed_models_list:
        table_data = []

        for m in installed_models_list:
            card = get_model_card(m["model"], ollama_info=m)
            info = get_model_info(card["name"]) or {}
            stats = info.get("benchmark_stats", {})
            model_langs = info.get("langs", [])

            # --- FILTRAGE ---
            if selected_langs and (
                not model_langs or not set(selected_langs).issubset(set(model_langs))
            ):
                continue

            keep_model = True
            for cap_label in selected_caps:
                key = cap_options[cap_label]
                if (
                    (
                        key == "tools_validated"
                        and stats.get("tool_capability", {}).get("success_rate", 0) < 0.9
                    )
                    or (
                        key == "json_validated"
                        and stats.get("json_capability", {}).get("schema_compliance_rate", 0) < 0.9
                    )
                    or (
                        key == "reasoning_high"
                        and stats.get("quality_scores", {}).get("reasoning_avg", 0) < 0.6
                    )
                    or key == "fast_ux"
                    and stats.get("avg_ttft_ms", 9999) > 800
                ):
                    keep_model = False
            if not keep_model:
                continue

            # --- NORMALISATION (TYPES NUM√âRIQUES POUR TRI) ---

            # Vitesse (Float)
            speed_val = stats.get("avg_tokens_per_second", 0.0)
            if speed_val == 0.0:
                # Tentative de parsing du statique "35 t/s" si benchmark manquant
                try:
                    speed_val = float(card["metrics"]["speed"].split(" ")[0])
                except Exception:
                    speed_val = 0.0

            # TTFT (Float sec)
            ttft_ms = stats.get("avg_ttft_ms", 0)
            ttft_s = ttft_ms / 1000.0 if ttft_ms else None

            # RAM (Float)
            # Priorit√© : Benchmark > Taille Disque > 0
            ram_val = stats.get("ram_usage_at_max_ctx_gb", 0.0)
            if ram_val == 0.0:
                ram_val = _parse_size_to_float(card.get("size_str", ""))

            # Params (Float)
            raw_pt = info.get("params_tot", card["specs"]["params"])
            raw_pa = info.get("params_act", "‚Äî")
            val_pt = _parse_params_to_float(raw_pt)
            val_pa = _parse_params_to_float(raw_pa)

            # Taille Disque (Float)
            disk_val = _parse_size_to_float(card.get("size_str", ""))

            # Caps (String + Count pour pr√©-tri)
            caps_icons = []
            if stats.get("tool_capability", {}).get("success_rate", 0) > 0.9:
                caps_icons.append("üõ†Ô∏è")
            if stats.get("json_capability", {}).get("schema_compliance_rate", 0) > 0.9:
                caps_icons.append("{}")
            if stats.get("quality_scores", {}).get("reasoning_avg", 0) > 0.7:
                caps_icons.append("üß†")

            # UX
            ux_rating = stats.get("ux_rating", "")
            ux_emoji = ux_rating.split(" ")[0] if ux_rating else ""

            # CO2
            co2_kg = stats.get("avg_co2_per_1k_tokens", 0)
            co2_mg = co2_kg * 1_000_000 if co2_kg else None

            # Nettoyage Licence (Remplacement des erreurs par un emoji)
            raw_lic = stats.get("detected_license", "‚Äî")
            if raw_lic in ["Erreur lecture", "Non d√©tect√©e", "Erreur", "N/A"]:
                lic_display = "‚ùì"
            else:
                lic_display = raw_lic

            row = {
                "Type": "‚òÅÔ∏è API" if card["is_cloud"] else "üíª Local",
                "Nom": card["name"],
                "Licence": lic_display,
                "UX": ux_emoji,
                "Vitesse": speed_val,
                "TTFT": ttft_s,
                "Efficience": stats.get("efficiency_grade", "‚Äî"),
                "CO2": co2_mg,
                "Caps": " ".join(caps_icons),
                "RAM": ram_val,
                "Contexte": int(info.get("ctx", 0)) if str(info.get("ctx", "0")).isdigit() else 0,
                "Params Tot.": val_pt,
                "Params Act.": val_pa,
                "Disque": disk_val,
                "Langues": model_langs,
                "Link": info.get("link"),
                # M√©triques cach√©es pour le pr√©-tri par d√©faut
                "_count_caps": len(caps_icons),
                "_count_langs": len(model_langs),
            }
            table_data.append(row)

        if table_data:
            df = pd.DataFrame(table_data)

            # PR√â-TRI PAR D√âFAUT : Par Capacit√©s puis par Nombre de Langues (Descendant)
            df = df.sort_values(by=["_count_caps", "_count_langs"], ascending=False)

            st.dataframe(
                df,
                column_order=[
                    "Type",
                    "Nom",
                    "Licence",
                    "UX",
                    "Vitesse",
                    "TTFT",
                    "Efficience",
                    "CO2",
                    "Caps",
                    "RAM",
                    "Contexte",
                    "Params Tot.",
                    "Params Act.",
                    "Disque",
                    "Langues",
                    "Link",
                ],
                column_config={
                    "Type": st.column_config.TextColumn("Type", width="small"),
                    "Nom": st.column_config.TextColumn("Mod√®le", width="medium"),
                    "Licence": st.column_config.TextColumn(
                        "Licence",
                        width="small",
                        help="Licence d√©tect√©e automatiquement. Cliquez sur 'Lien' pour v√©rifier les conditions d'usage.",
                    ),
                    "UX": st.column_config.TextColumn(
                        "UX",
                        width="small",
                        help="Note de fluidit√© ressentie (bas√©e sur le TTFT).\n‚ö° Instantan√© (<300ms)\nüöÄ Rapide (<800ms)\nüê¢ Acceptable (<1.5s)",
                    ),
                    "Vitesse": st.column_config.NumberColumn(
                        "Vit. (t/s)",
                        format="%.1f t/s",
                        help="D√©bit de g√©n√©ration (Tokens/seconde). Valeur plus haute = g√©n√©ration plus rapide.",
                    ),
                    "TTFT": st.column_config.NumberColumn(
                        "Latence",
                        format="%.2f s",
                        help="Time To First Token : Temps d'attente avant le d√©but de la r√©ponse.",
                    ),
                    "Efficience": st.column_config.TextColumn(
                        "RSE",
                        width="small",
                        help="Grade (üü¢/üü°/üî¥) calcul√© selon le ratio : Qualit√© du Raisonnement / Co√ªt Carbone.",
                    ),
                    "CO2": st.column_config.NumberColumn(
                        "CO‚ÇÇ (mg)",
                        format="%.1f mg",
                        help="Impact carbone pour 1000 tokens g√©n√©r√©s (milligrammes).",
                    ),
                    "Caps": st.column_config.TextColumn(
                        "Caps.",
                        width="small",
                        help="Capacit√©s valid√©es :\nüõ†Ô∏è = Tool Calling fiable\n{} = JSON Schema respect√©\nüß† = Raisonnement logique > 70%",
                    ),
                    "RAM": st.column_config.NumberColumn(
                        "RAM",
                        format="%.1f GB",
                        help="M√©moire vive r√©elle consomm√©e. Si 0 (erreur de sonde), affiche la taille disque.",
                    ),
                    "Contexte": st.column_config.NumberColumn(
                        "Ctx (tk)",
                        format="%d",
                        help="Fen√™tre de contexte maximale (m√©moire √† court terme du mod√®le).",
                    ),
                    "Params Tot.": st.column_config.NumberColumn(
                        "P. Tot",
                        format="%.1f B",
                        help="Nombre total de param√®tres (Milliards). Indique la 'culture g√©n√©rale' du mod√®le.",
                    ),
                    "Params Act.": st.column_config.NumberColumn(
                        "P. Act",
                        format="%.1f B",
                        help="Param√®tres actifs par token (pour les mod√®les MoE). Indique le co√ªt d'inf√©rence r√©el.",
                    ),
                    "Disque": st.column_config.NumberColumn(
                        "Disque",
                        format="%.1f GB",
                        help="Espace de stockage occup√© sur le disque dur.",
                    ),
                    "Langues": st.column_config.ListColumn(
                        "Langues",
                        help="Liste des langues valid√©es par le benchmark (Compr√©hension ou G√©n√©ration).",
                    ),
                    "Link": st.column_config.LinkColumn("Lien", display_text="üîó"),
                },
                use_container_width=True,
                hide_index=True,
            )
        else:
            st.info("Aucun mod√®le ne correspond √† vos crit√®res.")
    else:
        st.warning("Aucun mod√®le d√©tect√© en local.")

    # ... (Reste du code de t√©l√©chargement inchang√©) ...
    st.markdown("---")
    st.markdown("### ‚¨áÔ∏è T√©l√©charger un nouveau mod√®le")

    col_select, col_info = st.columns([1, 1])
    with col_select:
        suggestions = sorted(get_all_friendly_names(local_only=True))
        options = (
            ["‚ú® S√©lectionner une suggestion..."] + suggestions + ["üõ†Ô∏è Autre (Saisie Manuelle)"]
        )
        choice = st.selectbox("Catalogue Wavestone", options)

        target_model_tag = ""
        if choice == "üõ†Ô∏è Autre (Saisie Manuelle)":
            target_model_tag = st.text_input("Tag Ollama", "")
            st.caption("[Ollama Library](https://ollama.com/library)")
        elif choice != "‚ú® S√©lectionner une suggestion...":
            info = get_model_info(choice)
            if info:
                target_model_tag = info["ollama_tag"]

    with col_info:
        if choice not in ["‚ú® S√©lectionner une suggestion...", "üõ†Ô∏è Autre (Saisie Manuelle)"] and (
            info := get_model_info(choice)
        ):
            st.info(f"**{info.get('desc', '')}**")
            st.markdown(
                f"**Contexte:** `{info.get('ctx', '?')}` | **Params:** `{info.get('params_tot', '?')}`"
            )

    st.write("")
    if st.button("‚¨áÔ∏è Lancer le t√©l√©chargement") and target_model_tag:
        status = st.status(f"T√©l√©chargement de {target_model_tag}...", expanded=True)
        pbar = status.progress(0, text="Connexion...")
        try:
            for progress in LLMProvider.pull_model(target_model_tag):
                if progress.get("total"):
                    p = progress["completed"] / progress["total"]
                    pbar.progress(p, text=f"{progress['status']} - {int(p*100)}%")
                else:
                    pbar.progress(0.5, text=progress["status"])
            pbar.progress(1.0, text="Termin√© !")
            status.update(label="‚úÖ Succ√®s !", state="complete", expanded=False)
            time.sleep(1)
            st.rerun()
        except Exception as e:
            status.update(label="‚ùå Erreur", state="error")
            st.error(str(e))
